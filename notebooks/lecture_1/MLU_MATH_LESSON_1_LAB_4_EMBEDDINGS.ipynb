{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<div style=\"background-image: linear-gradient(145deg, rgba(35, 47, 62, 1) 0%, rgba(0, 49, 129, 1) 40%, rgba(32, 116, 213, 1) 60%, rgba(244, 110, 197, 1) 85%, rgba(255, 173, 151, 1) 100%); padding: 1rem 2rem; width: 95%\"><img style=\"width: 60%;\" src=\"../../images/MLU_logo.png\"></div>\n",
    "\n",
    "# <a name=\"0\">MLU Mathematical Fundamentals for Machine Learning</a>\n",
    "# <a name=\"0\">Lecture 1: Basic linear algebra</a>\n",
    "## <a name=\"0\">Lab 1.4: Vector spaces and embeddings in NLP and LLMs</a>\n",
    "\n",
    " 1. <a href=\"#1\">LLM Embeddings as an application of vector spaces</a> \n",
    " 2. <a href=\"#2\">Distance and similarity in vector space</a>\n",
    " 3. <a href=\"#3\">Analogy reasoning</a>\n",
    "\n",
    "Vector spaces are a core concept in linear algebra, with applications in various machine learning algorithms. In this notebook, we will explore the definition, properties, and applications of vector spaces. We will use Large Language Model (LLM) embeddings as a practical example to illustrate these concepts.\n",
    "\n",
    "A vector space is a collection of objects, called vectors, that can be added together and multiplied by scalars. Formally, a vector space $V$ over a field $F$ is a set equipped with two operations:\n",
    "\n",
    "1. **Vector Addition**: For any vectors $\\mathbf{u}, \\mathbf{v} \\in V$, there exists a vector $\\mathbf{u} + \\mathbf{v} \\in V$.\n",
    "2. **Scalar Multiplication**: For any scalar $c \\in F$ and vector $\\mathbf{v} \\in V$, there exists a vector $c\\mathbf{v} \\in V$.\n",
    "\n",
    "These operations must satisfy certain axioms, such as associativity, commutativity, and distributivity.\n",
    "\n",
    "Vector spaces have several important properties:\n",
    "\n",
    "1. **Closure**: The sum of any two vectors in the space is also in the space.\n",
    "2. **Zero Vector**: There exists a zero vector $\\mathbf{0}$ such that $\\mathbf{v} + \\mathbf{0} = \\mathbf{v}$ for any vector $\\mathbf{v}$.\n",
    "3. **Inverse**: For every vector $\\mathbf{v}$, there exists a vector $-\\mathbf{v}$ such that $\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}$.\n",
    "4. **Scalar Multiplication**: The product of any scalar and any vector is also in the space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upgrade libraries\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## <a name=\"1\">1. LLM Embeddings as an application of vector spaces</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Many NLP constructs, such as Large Language Models (LLMs), are based on the concept of **embeddings**, which are vector representations of words, sentences, or documents. These embeddings live in a high-dimensional vector space, where each dimension captures some aspect of the meaning or context of the text.\n",
    "\n",
    "For example, consider a simple LLM that generates embeddings for words. Each word is represented as a vector in a high-dimensional space, where the dimensions correspond to different semantic features. These embeddings can be added, subtracted, and scaled, making the space of embeddings a vector space.\n",
    "\n",
    "Which [Amazon Bedrock](https://aws.amazon.com/bedrock/), we have access to various embedding models, for instance the [Amazon Titan embedding models](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html). [This article](https://aws.amazon.com/jp/blogs/machine-learning/get-started-with-amazon-titan-text-embeddings-v2-a-new-state-of-the-art-embeddings-model-on-amazon-bedrock/) explains in detail what the state-of-the-art Titan models are capable of. We will use Amazon Titan Text Embeddings V2 in the demo below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Bedrock client using boto3\n",
    "bedrock = boto3.client(\"bedrock\")\n",
    "\n",
    "# Initialize the Bedrock runtime client\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here's an example of the embedding produced by Amazon Titan Text Embeddings V2 on a word. The embedding dimension, i.e. the length of the embedding vector, is a characteristic of every embedding model and is typically on the order of magnitude of the thousands for current state-of-the-art models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the word for which we want to generate an embedding\n",
    "word = \"amazing\"\n",
    "payload = {\"inputText\": word}\n",
    "\n",
    "# Invoke the model to generate the embedding\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    modelId=\"amazon.titan-embed-text-v2:0\",\n",
    "    body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "# Parse the response body from the model invocation\n",
    "body = json.loads(response.get('body').read())\n",
    "\n",
    "print(f\"The vector space has {len(body['embedding'])} dimensions\\n\")\n",
    "\n",
    "# Printing the first 10 components of the embedding vector\n",
    "print(f\"Printing the first 10 components of the embedding vector for word '{word}':\\n\\n{body['embedding'][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## <a name=\"2\">2. Distance and similarity in vector space</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Vectors are often depicted as arrows pointing in a certain direction within a space. They can also be represented numerically through the list of their components on a basis, which makes mathematical operations easier.\n",
    "\n",
    "The purpose of vector embeddings is to map complex entities like text or images into a vector space, allowing machine learning models to identify patterns and relationships. These embeddings convert non-numeric data into a numerical format while preserving their semantic meanings and relationships. This enables computational models in machine learning and natural language processing (NLP) to recognize similarities and differences between entities.\n",
    "\n",
    "In a vector space embedding, similar entities are placed close to each other, reflecting their semantic or contextual similarity. For example, in word embeddings, words with similar meanings are positioned near each other in the vector space. This spatial arrangement helps embeddings capture and organize the semantic relationships between entities, a concept known as the semantic space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Let's create a function that returns the embedding given a text input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to compute embeddings from text input\n",
    "def get_embedding(text, model=\"amazon.titan-embed-text-v2:0\"):\n",
    "    payload = {\"inputText\": text}\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=model,\n",
    "        body=json.dumps(payload)\n",
    "    )\n",
    "    body = json.loads(response.get(\"body\").read())\n",
    "    \n",
    "    # Return np array as it's easier to deal with\n",
    "    return np.array(body[\"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing it\n",
    "emb1 = get_embedding(\"cat\")\n",
    "print(f\"The Python type of the embedding vector is: {type(emb1)}\\nThe array shape is: {emb1.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "In the context of embeddings, semantically similar strings (words, phrases, or sentences) are represented by vectors that are \"close\" to each other in the embedding space. This concept is fundamental to many natural language processing (NLP) tasks, such as word similarity, sentence similarity, and clustering. The concept of \"closeness\" in the embedding space can be measured using various distance metrics, with Euclidean distance and cosine similarity being two of the most commonly used methods.\n",
    "\n",
    "* **Euclidean distance** measures the straight-line distance between two points in the embedding space. A small value of the distance between two words embeddings means that the words are close to each other.\n",
    "* **Cosine similarity** measures the cosine of the angle between two vectors. It is particularly useful for high-dimensional spaces because it focuses on the orientation of the vectors rather than their magnitude, and it is bounded between -1 (diametrically opposed vectors) to 1 (parallel vectors). When two word embeddings are close to each other, the cosine similarity is large. \n",
    "\n",
    "I.e. words or sentences that are close will have: \n",
    "- small Euclidean distance\n",
    "- large cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Let's create two functions that return the above metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    # Calculate the dot product\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "\n",
    "    # Calculate the norms of the vectors\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "\n",
    "    # Calculate the cosine similarity\n",
    "    cosine_sim = dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "    return cosine_sim\n",
    "\n",
    "\n",
    "def euclidean_distance(vector1, vector2):\n",
    "    # Calculate the difference between the vectors\n",
    "    difference = vector1 - vector2\n",
    "\n",
    "    # Calculate the Euclidean distance\n",
    "    euclidean_dist = np.linalg.norm(difference)\n",
    "\n",
    "    return euclidean_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Let's test them for concepts that are semantically approaching each other:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "<img src=\"../../images/vector_similarity.png\" alt=\"vector_similarity\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = [\"mouse\", \"pizza\", \"rome\", \"italian\", \"Italy\", \"italy\"]\n",
    "base_word = \"italy\"\n",
    "\n",
    "# Construct dataframe to store data\n",
    "df = pd.DataFrame(words, columns=[\"word_1\"])\n",
    "df[\"word_2\"] = base_word\n",
    "\n",
    "# Compute distance metrics for each pair of words\n",
    "df[\"cosine_similarity\"] = df.apply(lambda r: cosine_similarity(get_embedding(r.word_1), get_embedding(r.word_2)), axis=1)\n",
    "df[\"euclidean_distance\"] = df.apply(lambda r: euclidean_distance(get_embedding(r.word_1), get_embedding(r.word_2)), axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Let's repeat the same exercise. This time, instead of just single words, we use sentences. Let's see how the Cosine similarity and Euclidean distance compare when used to compare a human (positive) review with:\n",
    "* a similar review generated by an LLM\n",
    "* a negative review \n",
    "* an irrelevant review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 80%; max-height:80%; margin: 5px;\" src=\"../../images/MLU_challenge.png\" alt=\"MLU challenge\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Exercise 1.</b> In the next cell you find an asin description made by a human, a similar description made by an LLM, a description with negative tone, and an irrelevant description. Compute cosine similarity and Euclidean distance for the human description against the other 3, and check if the results align with your expectation.\n",
    "</p>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_description = \"This laptop has a sleek design, high-resolution display, and long battery life.\"\n",
    "llm_description = \"Featuring a slim design, this laptop offers a vibrant screen and extended battery duration.\"\n",
    "bad_description = \"The laptop lacks in battery duration, its display resolution is lacking, and the appearance is bulky.\"\n",
    "irrelevant_description = \"It was a sunny and crisp day in the village located right on the Amalfi coast.\" \n",
    "\n",
    "###### YOUR CODE HERE ######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### END OF CODE ######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "<div style=\"align: left; border: 4px solid lightcoral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_question.png\" alt=\"MLU solution\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><br/><b>Challenge Help</b></p>\n",
    "        <p><br/><br/>If you're stuck, remove the <code>#</code> before the <code>load</code> instruction in the next code cell to display sample solutions.</p>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/lab14_ex1_solutions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## <a name=\"3\">3. Analogy reasoning</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "As elements of a vector space, the usual operations on embeddings are well defined. By performing vector operations on embeddings, we can uncover interesting relationships and patterns within the language. \n",
    "\n",
    "One such application is **analogy reasoning** to find words that are related in a similar way to a given pair of words. For instance, in the classic example `king - man + female = queen` the embedding for \"queen\" can be approximated by subtracting the embedding for \"man\" from the embedding for \"king\" and then adding the embedding for \"female.\" This demonstrates the ability of LLM embeddings to capture subtle semantic nuances and perform complex linguistic tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "In the next cell, we give a demonstration of analogy reasoning. We create a function `analogy_reasoning` that performs the following steps:\n",
    "\n",
    "* Retrieves embeddings for the given query and target words.\n",
    "* Calculates an analogy vector based on the query words.\n",
    "* Computes cosine similarity and Euclidean distance between the analogy vector and each target word's embedding.\n",
    "* Identifies the closest target word based on the similarity measures.\n",
    "* The function returns a tuple containing the closest target word based on both cosine similarity and Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analogy_reasoning(query_words, target_words, get_embedding_func):\n",
    "    \"\"\"\n",
    "    Performs analogy reasoning using LLM embeddings.\n",
    "\n",
    "    Args:\n",
    "    query_words: A list of query words for the analogy.\n",
    "    target_words: A list of target words to compare.\n",
    "    get_embedding_func: A function to get the embedding for a word.\n",
    "\n",
    "    Returns:\n",
    "    A tuple containing the closest target word based on cosine similarity and Euclidean distance.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get embeddings for query and target words\n",
    "    query_embs = np.array([get_embedding_func(word) for word in query_words])\n",
    "    target_embs = np.array([get_embedding_func(word) for word in target_words])\n",
    "\n",
    "    # Calculate analogy vector\n",
    "    analogy_vec = query_embs[0] - query_embs[1] + query_embs[2]\n",
    "\n",
    "    # Calculate cosine similarity and Euclidean distance\n",
    "    cos_sim = np.array([cosine_similarity(target, analogy_vec) for target in target_embs])\n",
    "    euc_dist = np.array([euclidean_distance(target, analogy_vec) for target in target_embs])\n",
    "\n",
    "    # Find closest target based on cosine similarity and Euclidean distance\n",
    "    closest_index_cos = np.argmax(cos_sim)\n",
    "    closest_index_dist = np.argmin(euc_dist)\n",
    "    \n",
    "    print(f\"Cosine similarity   |  {query_words[0]} - {query_words[1]} + {query_words[2]} ~ {target_words[closest_index_cos]}\")\n",
    "    print(f\"Euclidean distance  |  {query_words[0]} - {query_words[1]} + {query_words[2]} ~ {target_words[closest_index_dist]}\")\n",
    "\n",
    "    return target_words[closest_index_cos], target_words[closest_index_dist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Trying to find the closest word embedding from a whole vocabulary would be very costly. Thus, we will choose a handful of words as targets and let the `analogy_reasoning` function find the closest among that set. \n",
    "\n",
    "Let's first check that the `king - man + female ~ queen` analogy holds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_words = ['king', 'man', 'female']\n",
    "target_words = ['queen', 'lawyer', 'professor', 'dancer', 'mother', 'father', 'apple', 'pear', 'orange', 'red', 'blue']\n",
    "\n",
    "result = analogy_reasoning(query_words, target_words, get_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 80%; max-height:80%; margin: 5px;\" src=\"../../images/MLU_challenge.png\" alt=\"MLU challenge\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Exercise 2.</b> Using the provided analogy_reasoning function, create a list of 3 analogy problems (e.g., \"king - man + female = queen\"). For each problem, provide a list of potential target phrases. Test the function on these analogies and report the results.\n",
    "</p>\n",
    "        <p>Here're some ideas for topics:\n",
    "            <ul>\n",
    "                <li>Famous singers and their most iconic songs.</li>\n",
    "                <li>Countries and their capitals.</li>\n",
    "                <li>Words in different languages</li>\n",
    "            <ul>\n",
    "        </p>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### END OF CODE ######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "<div style=\"align: left; border: 4px solid lightcoral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_question.png\" alt=\"MLU solution\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><br/><b>Challenge Help</b></p>\n",
    "        <p><br/><br/>If you're stuck, remove the <code>#</code> before the <code>load</code> instruction in the next code cell to display sample solutions.</p>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/lab14_ex2_solutions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: left; background-color:#330066; width:99%;\"> \n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_robot.png\" alt=\"MLU robot\" width=\"100\" height=\"100\"/>\n",
    "    <span style=\"color: white; padding-left: 10px; align: left; margin: 15px;\">\n",
    "        <h3>Congratulations!</h3>\n",
    "        You have completed Lab 1.4: Vector spaces and embeddings in NLP and LLMs of Lecture 1: Basic linear algebra of MLU Mathematical Fundamentals of Machine Learning.\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
