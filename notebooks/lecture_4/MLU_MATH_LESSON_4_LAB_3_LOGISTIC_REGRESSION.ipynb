{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<div style=\"background-image: linear-gradient(145deg, rgba(35, 47, 62, 1) 0%, rgba(0, 49, 129, 1) 40%, rgba(32, 116, 213, 1) 60%, rgba(244, 110, 197, 1) 85%, rgba(255, 173, 151, 1) 100%); padding: 1rem 2rem; width: 95%\"><img style=\"width: 60%;\" src=\"../../images/MLU_logo.png\"></div>\n",
    "\n",
    "# <a name=\"0\">MLU Mathematical Fundamentals for Machine Learning</a>\n",
    "# <a name=\"0\">Lecture 4: Differential calculus</a>\n",
    "## <a name=\"0\">Lab 4.1: Logistic Regression</a>\n",
    "\n",
    " 1. <a href=\"#1\">Data for binary classification</a> \n",
    " 2. <a href=\"#2\">Gradient descent for logistic regression</a> \n",
    " 3. <a href=\"#3\">Model evaluation and sklearn comparison</a>\n",
    " 4. <a href=\"#4\">Logistic regression on higher-dimensional data</a>\n",
    " \n",
    "[**Logistic regression**](https://en.wikipedia.org/wiki/Logistic_regression) is a fundamental classification algorithm in machine learning. Despite its name, it's used for binary classification problems. The algorithm models the probability that an instance belongs to a particular class.\n",
    "\n",
    "Key points about logistic regression are:\n",
    " - The decision boundary is linear on the data features and the parameters (weights).\n",
    " - It uses the logistic function (sigmoid) to map predictions to probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade libraries\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import torch \n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## <a name=\"1\">1. Data for binary classification</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "For this logistic regression exercise we will reuse data from a previous lab. Fashion-MNIST is a dataset of Zalando's product images, consisting of a data set of 70,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes corresponding to fashion items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch dataset\n",
    "data = fetch_openml(name=\"Fashion-MNIST\")\n",
    "# Assemble features and target in same DataFrame for easy data handling\n",
    "df = data[\"data\"]\n",
    "# Store target as integer\n",
    "df[\"target\"] = data[\"target\"].astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Selecting a subset of two classes\n",
    "\n",
    "Since this dataset is multi-class, we cannot use it fully for a binary classification problem. But we can use a subset of it, if we select two out of the ten available fashion item categories. \n",
    "\n",
    "Let's for instance pick classes `\"Sandal\"` and `\"Sneaker\"`, but this can be changed by any other 2 classes that you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The 10 classes are 0 to 9 and represent the types of items on the table above\n",
    "label_description = {\n",
    "    0: \"T-shirt/top\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\"\n",
    "}\n",
    "\n",
    "chosen_labels = (\"Sandal\", \"Sneaker\")\n",
    "chosen_classes = [list(label_description.values()).index(c) for c in chosen_labels]\n",
    "\n",
    "df_binary = df[df.target.isin(chosen_classes)]\n",
    "print(f\"Selected classes: {[label_description[c] for c in df_binary.target.unique()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Dimensionality reduction and feature normalization\n",
    "\n",
    "The Fashion-MNIST datapoints are 784-dimensional, black-and-white, 28x28 pixel images. While we could apply Logistic Regression to all those features, let's simplily by using a lower-dimensional representation of the dataset. Recall that we can apply PCA to the data points and project them to retain only a few of their principal components. \n",
    "\n",
    "Let's run PCA with `n_components=2` to transform the 784-dimensional data points into 2-dimensional entities and use those as features of the logistic regression modal. \n",
    "\n",
    "Recall that data has to be normalized before PCA can be applied. The code below is similar to that of Lab 2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Make a copy of the data so that pop doesn't overwrite df\n",
    "df_ = df_binary.copy()\n",
    "\n",
    "# Remove the target as we'll only scale the features\n",
    "y = df_.pop(\"target\").values\n",
    "\n",
    "# Transform labels from the chosen ones to 0, 1 to run binary classification later\n",
    "y = np.array([0 if i==chosen_classes[0] else 1 for i in y])\n",
    "\n",
    "# X values\n",
    "X = df_.values\n",
    "\n",
    "# Scaled features with zero mean and unit variance\n",
    "X_sc = scaler.fit_transform(X)\n",
    "\n",
    "# Shape of the input data\n",
    "print(f\"Shape of features matrix: {X_sc.shape}\")\n",
    "\n",
    "# Initialize PCA object\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA to normalized data\n",
    "X_pca = pca.fit_transform(X_sc)\n",
    "\n",
    "# Shape of the data after PCA\n",
    "print(f\"Shape of the features matrix after PCA: {X_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Next, we split the data into train and test to be able to evaluate the train model on unseen data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "**Feature scaling** is advisable to solve logistic regression via gradient descent as it ensures all features contribute proportionally to the model, regardless of their original units or ranges. \n",
    "\n",
    "This normalization leads to faster and more stable convergence during optimization, prevents any single feature from dominating due to its scale, and avoids numerical instabilities in calculations, particularly with exponential functions. Scaled features also enhance the interpretability of model coefficients, and ensure consistent performance with regularization techniques. \n",
    "\n",
    "Let's fit the scaler on the training data and transform both the train and test datasets to prevent data leakage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply feature scaling to the low-dimensionality PCA data\n",
    "scaler2 = StandardScaler()\n",
    "\n",
    "X_train = scaler2.fit_transform(X_train)\n",
    "X_test = scaler2.transform(X_test)\n",
    "\n",
    "print(f\"Train data mean = {X_train.mean()}, std = {X_train.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the projected, scaled data\n",
    "for cl in (0, 1):\n",
    "    plt.scatter(X_train[y_train==cl][:, 0], X_train[y_train==cl][:, 1], s=2, alpha=0.7, label=chosen_labels[cl])\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Training Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## <a name=\"2\">2. Gradient descent for logistic regression</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "While logistic regression can be solved using other methods (like maximum likelihood estimation), we will use gradient descent because:\n",
    "- It's a general optimization technique applicable to many machine learning problems, including the final project.\n",
    "- It can handle large datasets efficiently, especially with variations like stochastic gradient descent.\n",
    "- It provides insight into the learning process and serves as an introduction to neural networks.\n",
    "\n",
    "For our gradient descent implementation from scratch, we will use the same matrix convention from the OLS lab in Lecture 2. \n",
    "$$\n",
    "\\mathbf{X} = \\begin{pmatrix}\n",
    "1 & x_{11} & \\dots  & x_{1m}\\\\\n",
    "1 & x_{21} & \\dots  & x_{2m} \\\\\n",
    "\\vdots & \\vdots & \\dots & \\vdots\\\\\n",
    "1 & x_{n1}  & \\dots  & x_{nm}\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "The design matrix provides a concise way to represent all input features for all samples in a single matrix. In gradient descent, this representation allows efficient computation of gradients, because all weights, including the bias or intercept, are stored in one tensor variable. \n",
    "\n",
    "The logistic regression model can be then written as:\n",
    "$$\n",
    "\\hat{y} = \\sigma(Xw) = \\frac{1}{1 + e^{-Xw}}\n",
    "$$\n",
    "\n",
    "Below we construct the design matrix for our data and also convert it to PyTorch tensors, as that will be needed to implement the gradient descent loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assemble design matrix\n",
    "def assemble_design_matrix(X):\n",
    "    return np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "# Construct design matrix for the train and test splits\n",
    "X_train_bias = assemble_design_matrix(X_train)\n",
    "X_test_bias = assemble_design_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_bias_tensor = torch.FloatTensor(X_train_bias)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_test_bias_tensor = torch.FloatTensor(X_test_bias)\n",
    "y_test_tensor = torch.FloatTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "We implement the sigmoid and the logistic regression function explicitely. Notice that you could also use native torch implementations [`torch.sigmoid`](https://pytorch.org/docs/stable/generated/torch.sigmoid.html) and [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) instead. \n",
    "\n",
    "The loss function in logistic regression is given by \n",
    "$$\n",
    "\\mathcal{L}(w) = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log\\left( \\sigma\\left( X w \\right)\\right) + (1 - y_i) \\log\\left( 1 - \\sigma\\left( X w \\right)\\right) \\right]\n",
    "$$\n",
    "\n",
    "This function can be easily implemented, however we will use here the [`torch.nn.functional.binary_cross_entropy`](https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy.html) which is more performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + torch.exp(-z))\n",
    "\n",
    "def logistic_regression(data, weights):\n",
    "    return sigmoid(torch.mm(data, weights))\n",
    "\n",
    "def binary_cross_entropy(y_pred, y_true):\n",
    "    return torch.nn.functional.binary_cross_entropy(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 80%; max-height:80%; margin: 5px;\" src=\"../../images/MLU_challenge.png\" alt=\"MLU challenge\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>It is your turn!</b></p>\n",
    "        <p><b>Exercise 1. Gradient descent for logistic regression.</b></p>\n",
    "        <p>Implement gradient descent to solve logistic regression. Follow these steps:\n",
    "            <ul>\n",
    "                <li>Initialize the weights <code>w</code> with small values using a normal distribution with mean 0 and standard deviation 0.01. This is a simple and often effective approach that will work in this case.</li>\n",
    "                <li>Set the learning rate and number of epochs as hyperparameters. We recommend learning rate 0.1 and 1000 or 2000 training epochs</li>\n",
    "                <li>Inside the training loop, perform a forward pass using the logistic regression function given above. This means computing the predictions of <code>logistic_regression</code> on the <code>X_train_bias_tensor</code> training data for the current weights.</li>\n",
    "                <li>Compute the binary cross-entropy loss using the formula above.</li>\n",
    "                <li>Perform a backward pass to calculate gradients.</li>\n",
    "                <li>Update the weights all at once using gradient descent.</li>\n",
    "                <li>Reset the gradients to zero.</li>\n",
    "                <li>Store the current training loss in a list <code>train_losses</code>.</li>\n",
    "                <li>Apply the model to the test data and store the test loss in a list <code>test_losses</code>.</li>\n",
    "                </ul>\n",
    "        </p>\n",
    "        </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### END OF CODE ######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "<div style=\"align: left; border: 4px solid lightcoral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_question.png\" alt=\"MLU solution\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Challenge Help</b></p>\n",
    "        <p>If you get an error computing the binary cross entropy, check that both input tensors are of the same shape. You can use <code>reshape</code> or <code>squeeze()</code>/<code>unsqueeze()</code> to ensure that.</p>\n",
    "        <p>If you're stuck, remove the <code>#</code> before the <code>load</code> instruction in the next code cell to display a sample solution.</p>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/lab43_ex1_solutions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "If you correctly stored the losses in `train_losses` and `test_losses`, you can visualize their evolution with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Raise errors if losses from Exercise 1 are not defined\n",
    "if \"train_losses\" not in dir():\n",
    "    raise NameError(\"Please define a `train_losses` variable containing the train losses during the gradient descent loop.\")\n",
    "if \"test_losses\" not in dir():\n",
    "    raise NameError(\"Please define a `test_losses` variable containing the test losses during the gradient descent loop.\")\n",
    "\n",
    "# Plot loss curve\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(train_losses, label=\"Train loss\")\n",
    "plt.plot(test_losses, label=\"Test loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## <a name=\"2\">3. Model evaluation and sklearn comparison</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "If your training proceeded correctly, you must have a vector `w` of weights with the 3 trained parameters of the model. To measure the performance of the model on unseen data, we apply it to the test data. Remember that the output of logistice regression is a probability between 0 and 1. To convert this to a class prediction, we typically use a threshold of 0.5, although this threshold could in theory be adjusted to another value, if that proved to perform better on the data.\n",
    "\n",
    "Let's apply the trained model to the test data set and measure the accuracy of the logistic regressor, defined as: \n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to map probabilities to classes (0, 1) for different thresholds. \n",
    "def proba_to_class(proba, threshold=0.5):\n",
    "    return (proba >= threshold).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Raise errors if variable and function from Exercise 1 are not defined\n",
    "if \"w\" not in dir():\n",
    "    raise NameError(\"Please define a `w` variable containing the solution to the logistic regression model.\")\n",
    "\n",
    "# PyTorch model evaluation\n",
    "with torch.no_grad():\n",
    "    y_test_pred_torch = logistic_regression(X_test_bias_tensor, w)\n",
    "    y_test_pred_torch = proba_to_class(y_test_pred_torch > 0.5)\n",
    "    accuracy_torch = accuracy_score(y_test_tensor, y_test_pred_torch)\n",
    "    print(f\"PyTorch Model Accuracy: {accuracy_torch:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Comparison with sklearn implementation\n",
    "\n",
    "Let's compare our custom gradient descent implementation with scikit-learn's built-in [`LogisticRegression`](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html) class. This is part of the `sklearn.linear_model` module and uses the `lbfgs` solver, which is an optimization algorithm in the family of quasi-Newton methods.\n",
    "\n",
    "The model is trained and used via the usual `sklearn` API for ML models, with `.fit()` and `.predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sklearn implementation\n",
    "sk_logreg = LogisticRegression()\n",
    "sk_logreg.fit(X_train, y_train)\n",
    "y_test_pred_sklearn = sk_logreg.predict(X_test)\n",
    "accuracy_sklearn = accuracy_score(y_test_pred_sklearn, y_test)\n",
    "print(f\"Sklearn Model Accuracy: {accuracy_sklearn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "tags": []
   },
   "source": [
    "If you trained your model correctly, both results should be close to each other.\n",
    "\n",
    "We can inspect the values of the parameters from both approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Raise errors if variable and function from Exercise 1 are not defined\n",
    "if \"w\" not in dir():\n",
    "    raise NameError(\"Please define a `w` variable containing the solution to the logistic regression model.\")\n",
    "\n",
    "grad_desc_params = [f\"{p:.4f}\" for p in w.detach().numpy().flatten().tolist()]\n",
    "print(f\"Weights found via gradient descent: {', '.join(grad_desc_params)}\")\n",
    "\n",
    "sklearn_params = sk_logreg.intercept_.tolist() + sk_logreg.coef_[0].tolist()\n",
    "sklearn_params = [f\"{p:.4f}\" for p in sklearn_params]\n",
    "print(f\"Weights found via sklearn solution: {', '.join(sklearn_params)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Visualize the decision boundary\n",
    "\n",
    "Finally, let's plot the decision boundary found by both methods, which for the 2-dimensional data is given by equation:\n",
    "$$\n",
    "w_0 + w_1 x + w_2 y = 0.\n",
    "$$\n",
    "\n",
    "We can plot this line as: \n",
    "$$\n",
    "y = \\frac{-(w_0 + w_1 x)}{w_2}.\n",
    "$$\n",
    "\n",
    "The code below creates a plot of the probability values in different areas of the space of features, together with the linear decision boundary that separates the regions of assigned class 0 and assigned class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize decision boundaries\n",
    "def plot_decision_boundary(X, y, model, title):\n",
    "    # Create 2D grid of points\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "    \n",
    "    # Prepare the grid points\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    # Predict probabilities\n",
    "    if isinstance(model, LogisticRegression):\n",
    "        # For sklearn use .predict_proba\n",
    "        Z = torch.FloatTensor(model.predict_proba(grid)[:, 1])\n",
    "        # The weights are in .intercept_ and .coef_\n",
    "        w0 = model.intercept_.tolist()\n",
    "        w1, w2 = model.coef_[0].tolist()\n",
    "        \n",
    "    elif isinstance(model, torch.Tensor):\n",
    "        # For grad descent the model is a tensor of weiths, use to predict\n",
    "        X_design = torch.FloatTensor(assemble_design_matrix(grid))\n",
    "        Z = logistic_regression(X_design, model)\n",
    "        Z = Z.detach().numpy()\n",
    "        # The weights are in model\n",
    "        w0, w1, w2 = [ww.detach().numpy() for ww in model]\n",
    "    else: \n",
    "        raise ValueError(\"Please enter either a LogisticRegression object or a torch.Tensor of weights as model.\")\n",
    "    \n",
    "    # Reshape values\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot contours of probability\n",
    "    plt.contourf(xx, yy, Z, alpha=0.2, cmap=cm.coolwarm)\n",
    "    \n",
    "    # Plot boundary line\n",
    "    y_boundary = -(w0 + w1*xx[0])/w2\n",
    "    plt.plot(xx[0], y_boundary, \"k-.\", lw=2)\n",
    "    \n",
    "    # Plot data points\n",
    "    mrks = [\"x\", \".\"]\n",
    "    clrs = [\"r\", \"b\"]\n",
    "    for cl in (0, 1):\n",
    "        plt.scatter(X[y==cl][:, 0], X[y==cl][:, 1], c=clrs[cl], s=20, marker=mrks[cl], alpha=0.7, label=chosen_labels[cl])\n",
    "\n",
    "    # Adjust plot limits\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Raise errors if variable and function from Exercise 1 are not defined\n",
    "if \"w\" not in dir():\n",
    "    raise NameError(\"Please define a `w` variable containing the solution to the logistic regression model.\")\n",
    "\n",
    "# Plot decision boundaries for both models\n",
    "plot_decision_boundary(X_test, y_test, w, \"PyTorch Model Decision Boundary\")\n",
    "plot_decision_boundary(X_test, y_test, sk_logreg, \"Sklearn Model Decision Boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## <a name=\"4\">4. Logistic regression on higher-dimensional data</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "The example above was for 2-dimensional data. The method works exactly the same if the number of features is larger. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 80%; max-height:80%; margin: 5px;\" src=\"../../images/MLU_challenge.png\" alt=\"MLU challenge\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>It is your turn!</b></p>\n",
    "        <p><b>Exercise 1. Logistic regression with more features.</b></p>\n",
    "        <p>Train a logistic regression model on more features. Re-do the PCA on the original Fashion-MNIST dataset to retain more principal components. Run the gradient descent algorithm on this higher-dimensional dataset.</p>\n",
    "        <p>Does the model achieve better or worse performance? What's a good number of features to use?</p>\n",
    "        </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### END OF CODE ######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"align: left; border: 4px solid lightcoral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_question.png\" alt=\"MLU solution\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Challenge Help</b></p>\n",
    "        <p>You can reuse the code above only changing the number of components of the PCA.</p>\n",
    "        <p>If you're stuck, remove the <code>#</code> before the <code>load</code> instruction in the next code cell to display a sample solution.</p>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/lab43_ex2_solutions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "As a stretch goal for this lab, you can go back to the label selection for the Fashion-MNIST data set and build models to classify any other pair of fashion items among the 10 different classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: left; background-color:#330066; width:99%;\"> \n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_robot.png\" alt=\"MLU robot\" width=\"100\" height=\"100\"/>\n",
    "    <span style=\"color: white; padding-left: 10px; align: left; margin: 15px;\">\n",
    "        <h3>Congratulations!</h3>\n",
    "        You have completed Lab 4.3: Logistic regression of Lecture 4: Differential calculus of MLU Mathematical Fundamentals of Machine Learning.\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
