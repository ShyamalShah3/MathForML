
def make_pca_to_data(n_components, X_orig, y_orig):
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X_orig)
    
    X_train, X_test, y_train, y_test = train_test_split(X_pca, y_orig, test_size=0.1, random_state=42)
    scaler2 = StandardScaler()
    X_train = scaler2.fit_transform(X_train)
    X_test = scaler2.transform(X_test)
    
    X_train_bias = assemble_design_matrix(X_train)
    X_test_bias = assemble_design_matrix(X_test)
    
    X_train_bias_tensor = torch.FloatTensor(X_train_bias)
    y_train_tensor = torch.FloatTensor(y_train)
    X_test_bias_tensor = torch.FloatTensor(X_test_bias)
    y_test_tensor = torch.FloatTensor(y_test)
    
    return X_train_bias_tensor, y_train_tensor, X_test_bias_tensor, y_test_tensor
    
    
def run_gradient_descent(X, y):
    input_dim = X.shape[1]
    ww = torch.normal(0, 0.01, (input_dim, 1), requires_grad=True)

    learning_rate = 0.1
    num_epochs = 2000
    for epoch in range(num_epochs):
        y_pred = logistic_regression(X, ww)
        loss = binary_cross_entropy(y_pred.squeeze(), y)
        loss.backward()
        with torch.no_grad():
            ww -= learning_rate * ww.grad 
        ww.grad.zero_()
    return ww

# Prepare higher dimensional data. Change to test others
n_components = 50
X_high_train, y_high_train, X_high_test, y_high_test = make_pca_to_data(n_components, X_sc, y)
# Run gradient descent
w_high = run_gradient_descent(X_high_train, y_high_train)

# Evaluate gradient descent model
with torch.no_grad():
    y_high_test_pred = logistic_regression(X_high_test, w_high)
    y_high_test_pred = proba_to_class(y_high_test_pred > 0.5)
    accuracy_torch = accuracy_score(y_high_test, y_high_test_pred)
    print(f"PyTorch Model Accuracy for {n_components} components: {accuracy_torch:.4f}")
    
# Evaluate Sklearn implementation
sk_high = LogisticRegression()
sk_high.fit(X_high_train, y_high_train)
print(f"Sklearn Model Accuracy with {n_components} components: {sk_high.score(X_high_test.numpy(), y_high_test.numpy()):.4f}")