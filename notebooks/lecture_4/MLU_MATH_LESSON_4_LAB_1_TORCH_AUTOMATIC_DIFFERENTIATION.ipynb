{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<div style=\"background-image: linear-gradient(145deg, rgba(35, 47, 62, 1) 0%, rgba(0, 49, 129, 1) 40%, rgba(32, 116, 213, 1) 60%, rgba(244, 110, 197, 1) 85%, rgba(255, 173, 151, 1) 100%); padding: 1rem 2rem; width: 95%\"><img style=\"width: 60%;\" src=\"../../images/MLU_logo.png\"></div>\n",
    "\n",
    "# <a name=\"0\">MLU Mathematical Fundamentals for Machine Learning</a>\n",
    "# <a name=\"0\">Lecture 4: Differential calculus</a>\n",
    "## <a name=\"0\">Lab 4.1: Torch and Automatic Differentiation</a>\n",
    "\n",
    " 1. <a href=\"#1\">Getting familiar with torch tensors</a> \n",
    " 2. <a href=\"#2\">Operations with torch tensors</a> \n",
    " 3. <a href=\"#3\">Automatric differentiation with autograd</a> \n",
    "\n",
    "[**PyTorch**](https://pytorch.org/) is an open-source deep learning framework primarily used for building and training neural networks. As an optimized tensor library for deep learning using GPUs and CPUs, it offers flexibility and high performance for machine learning tasks. Originally developed by Meta AI, PyTorch is now part of the Linux Foundation umbrella. PyTorch is one of the most popular deep learning frameworks. \n",
    "\n",
    "In this lab, you will get familiar with PyTorch and in particular will learn how its automatic differentiation module, `autograd`, works. With this knowledge, you'll be well equipped to implement the gradient descent algorithm. This will serve you when tackling the final project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upgrade libraries\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a name=\"0\">Getting familiar with torch tensors</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Tensors are the fundamental data structure in PyTorch. They are similar to NumPy arrays but can be used on GPUs for accelerated computing. Tensors can represent scalars, vectors, matrices, and higher-dimensional data.\n",
    "\n",
    "Let's create some tensors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0D tensor (scalar)\n",
    "scalar = torch.tensor(42)\n",
    "display(Markdown(\"#### Scalar tensor:\"))\n",
    "print(scalar)\n",
    "print(f\"Size: {scalar.size()}\")\n",
    "print(f\"Shape: {scalar.shape}\\n\")\n",
    "\n",
    "# 1D tensor (vector)\n",
    "vector = torch.tensor([1, 2, 3, 4, 5])\n",
    "display(Markdown(\"#### 1D tensor (vector):\"))\n",
    "print(vector)\n",
    "print(f\"Size: {vector.size()}\")\n",
    "print(f\"Shape: {vector.shape}\\n\")\n",
    "\n",
    "# 2D tensor (matrix)\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "display(Markdown(\"#### 2D tensor (matrix):\"))\n",
    "print(matrix)\n",
    "print(f\"Size: {matrix.size()}\")\n",
    "print(f\"Shape: {matrix.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Pytorch tensors can also be created from a NumPy array, and be converted to a Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From NumPy array\n",
    "np_array = np.array([1, 2, 3, 4])\n",
    "x_from_np = torch.from_numpy(np_array)\n",
    "display(Markdown(\"#### Torch - NumPy conversion:\"))\n",
    "print(f\"Tensor from NumPy: {x_from_np}\")\n",
    "print(f\"Tensor back to NumPy: {x_from_np.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Tensor Initialization\n",
    "Random initialization is crucial in machine learning, particularly when working with neural networks. PyTorch provides several ways to create tensors with random values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a tensor with values uniformly distributed between 0 and 1\n",
    "uniform_tensor = torch.rand(1000)\n",
    "\n",
    "# Create a tensor with values uniformly distributed between -3 and 3\n",
    "uniform_scaled = torch.rand(1000) * 6 - 3  # scales [0,1] to [-3,3]\n",
    "\n",
    "# Visualize the distributions\n",
    "plt.figure(figsize=(8, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(uniform_tensor.numpy(), bins=30)\n",
    "plt.title('Uniform Distribution [0, 1]')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(uniform_scaled.numpy(), bins=30)\n",
    "plt.title('Uniform Distribution [-3, 3]')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 80%; max-height:80%; margin: 5px;\" src=\"../../images/MLU_challenge.png\" alt=\"MLU challenge\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>It is your turn!</b></p>\n",
    "        <p><b>Exercise 1. Gaussian initialization.</b></p>\n",
    "        <p>Initialize two torch tensors with 1000 points each, the first sampling from a standard normal distribution of mean 0 and standard deviation 1, and the second from a normal distribution of mean 2 and standard deviation 0.4.</p>\n",
    "        <p>Plot both distributions. Use the same <code>x_min</code>, <code>x_max</code> limits for the x axes in both plots so that you can see the relative size of one and the other. If you plot the density on the y axes, rather than the count, you'll be able to appreciate the difference between the two distributions more clearly.</p>\n",
    "        </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### END OF CODE ######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "<div style=\"align: left; border: 4px solid lightcoral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_question.png\" alt=\"MLU solution\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Challenge Help</b></p>\n",
    "        <p>You can use the function <code>torch.normal</code> to initialize the tensor. It takes as parameters the mean and std of the Guaussian distribution, together with a parameter <code>size</code> for the size of the returned object. To return a vector of dimension <code>dim</code>, you need to pass <code>size=(dim,)</code>.</p>\n",
    "        <p>If you're stuck, remove the <code>#</code> before the <code>load</code> instruction in the next code cell to display a sample solution.</p>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/lab41_ex1_solutions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## <a name=\"1\">Operations with torch tensors</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "PyTorch supports various operations on tensors. Here are some common ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic arithmetic operations\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "display(Markdown(\"#### Basic Operations:\"))\n",
    "print(a)\n",
    "print(b)\n",
    "print()\n",
    "print(\"Addition:\", a + b)  # or torch.add(a, b)\n",
    "print(\"Multiplication:\", a * b)  # element-wise multiplication, or torch.mul\n",
    "print(\"Subtraction:\", b - a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Matrix multiplication can be computed using different syntaxes, including the `@` operator, `torch.matmul()`, and `torch.mm()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat1 = torch.tensor([[1, 2], [3, 4]])\n",
    "mat2 = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "print(mat1)\n",
    "print(mat2)\n",
    "\n",
    "display(Markdown(\"#### Matrix Multiplication:\"))\n",
    "    \n",
    "# Using @ operator\n",
    "mat_mul = mat1 @ mat2\n",
    "print(f\"mat1 @ mat2:\\n{mat_mul}\")\n",
    "print()\n",
    "\n",
    "# Using torch.matmul function\n",
    "mat_mul_func = torch.matmul(mat1, mat2)\n",
    "print(f\"torch.matmul(mat1, mat2):\\n{mat_mul_func}\")\n",
    "print()\n",
    "\n",
    "# Using torch.mm function (only for 2D tensors)\n",
    "mat_mul_mm = torch.mm(mat1, mat2)\n",
    "print(f\"torch.mm(mat1, mat2):\\n{mat_mul_mm}\")\n",
    "\n",
    "display(Markdown(\"#### Matrix-Vector Multiplication:\"))\n",
    "vec = torch.tensor([1, 2])\n",
    "mat_vec_mul = mat1 @ vec\n",
    "print(f\"mat1 @ [1, 2]:\\n{mat_vec_mul}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a name=\"0\">Automatic Differentiation with autograd</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "One of the most powerful features of PyTorch is its automatic differentiation engine, known as Autograd. This system enables the efficient computation of gradients, which is crucial for training deep learning models.\n",
    "\n",
    "PyTorch Autograd tracks operations on tensors and builds a computational graph dynamically. Each tensor has a `.grad_fn` attribute that references a function that created the tensor. To compute gradients, we can call `.backward()` on a scalar tensor, and PyTorch will automatically compute the gradients for all tensors in the graph that require gradients.\n",
    "\n",
    "To use Autograd, you need to create tensors with `requires_grad=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a tensor with gradient tracking\n",
    "x = torch.tensor([2.0, 3.5], requires_grad=True)\n",
    "print(\"x:\", x)\n",
    "print(\"Requires gradient:\", x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "When we define a variable in terms of tensors that have gradients, PyTorch automatically sets up the computation graph for that variable. This allows us to compute gradients with respect to the input tensors. \n",
    "\n",
    "Let's explore this concept with an example:\n",
    "$$\n",
    "z = x^2 + y^3\n",
    "$$\n",
    "\n",
    "PyTorch automatically creates a computation graph for this operation, and `z` knows that it was created as a result of operations on `x` and `y`.\n",
    "\n",
    "The gradient of $z$ at each point is given by the vector of partial derivatives $(2 x, 3 y^2)$. At point $(x, y)=(2,2)$ the value of the gradient is $(4, 12)$.  \n",
    "\n",
    "The `.grad_fn` attribute of `z` shows the function that created this tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating tensors with gradient tracking\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Performing operations\n",
    "z = x**2 + y**3\n",
    "\n",
    "# Computing gradients\n",
    "z.backward()\n",
    "\n",
    "# Should be 4.0 (derivative of x^2 --> 2x at x=2)\n",
    "print(\"Gradient of z with respect to x:\", x.grad)\n",
    "# Should be 12.0 (derivative of y^3 --> 3y^2 at y=2)\n",
    "print(\"Gradient of z with respect to y:\", y.grad)  \n",
    "print()\n",
    "print(f\"z.grad_fn: {z.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "The sigmoid function, defined as: \n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "has a simple derivative that can be written in terms of the function:\n",
    "$$\n",
    "\\frac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 80%; max-height:80%; margin: 5px;\" src=\"../../images/MLU_challenge.png\" alt=\"MLU challenge\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>It is your turn!</b></p>\n",
    "        <p><b>Exercise 2. Sigmoid derivative.</b></p>\n",
    "        <p>Using pytorch autograd, compute and plot the derivative of the sigmoid function in the interval x = (-10, 10).</p>\n",
    "        <p>Compare with the analytical computation of the derivative.</p>\n",
    "        </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### END OF CODE ######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "<div style=\"align: left; border: 4px solid lightcoral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_question.png\" alt=\"MLU solution\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Challenge Help</b></p>\n",
    "        <p>Implement the sigmoid function to return a torch tensor. Apply the function to a tensor containing equally spaced points in the interval (-10, 10). Then compute the derivative by calling <code>torch.autograd.backward(y, torch.ones_like(x), create_graph=True)</code>, where <code>y</code> contains the values of the sigmoid on the desired interval. Read more about how to invoke <code>autograd</code> <a href=\"https://stackoverflow.com/questions/69148622/difference-between-autograd-grad-and-autograd-backward\">here</a>.</p>\n",
    "        <p>To be able to plot torch tensors with matplotlib, you need to detach the gradient and transform them into NumPy arrays with <code>.detach().numpy()</code>.\n",
    "        <p>If you're stuck, remove the <code>#</code> before the <code>load</code> instruction in the next code cell to display a sample solution.</p>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/lab41_ex2_solutions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: left; background-color:#330066; width:99%;\"> \n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_robot.png\" alt=\"MLU robot\" width=\"100\" height=\"100\"/>\n",
    "    <span style=\"color: white; padding-left: 10px; align: left; margin: 15px;\">\n",
    "        <h3>Congratulations!</h3>\n",
    "        You have completed Lab 4.1: Torch and Automatic Differentiation of Lecture 4: Differential Calculus of MLU Mathematical Fundamentals of Machine Learning.\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
