
# Perform PCA on the data
# Train on the train data and apply (transform) to the test data
pca = PCA()
X_pc_train = pca.fit_transform(X_train)
X_pc_test = pca.transform(X_test)

# Plot fraction of explained variance
plt.plot(pca.explained_variance_ratio_, "-o")
plt.title("Ratio of explained variance")
plt.xlabel("Number of principal components")
plt.ylabel("Explained variance ratio")
plt.show()

# Arrays to store results from PCR
r2_score_train_pcr, r2_score_test_pcr = [], []
rmse_train_pcr, rmse_test_pcr = [], []

# Perform PCR on an increasingly large number of Principal Components
for i in range (1, X_pc_train.shape[1]+1):
    lr_pcr = LinearRegression()
    
    # Take the first i PCs to train the Linear Regression
    X_train_pcr = X_pc_train[:,:i]
    X_test_pcr = X_pc_test[:,:i]

    lr_pcr.fit(X_train_pcr[:,:i], y_train)

    # Compute metrics from the fit
    r2_score_train_pcr.append(r2_score(y_train, lr_pcr.predict(X_train_pcr)))
    r2_score_test_pcr.append(r2_score(y_test, lr_pcr.predict(X_test_pcr)))
    
    rmse_train_pcr.append(root_mean_squared_error(y_train, lr_pcr.predict(X_train_pcr)))
    rmse_test_pcr.append(root_mean_squared_error(y_test, lr_pcr.predict(X_test_pcr)))
    
    
# Plot RMSE vs count of principal components used
plt.plot(rmse_train_pcr, "-o", label="PCR $RMSE_{train}$")
plt.plot(rmse_test_pcr, "-o", label="PCR $RMSE_{test}$")

# Plot horizontal lines at the values of the full linear regression model
plt.axhline(y=rmse_train, color='tab:blue', linestyle='--', label="LR $RMSE_{train}$")
plt.axhline(y=rmse_test, color='tab:orange', linestyle='--', label="LR $RMSE_{test}$")

plt.title("RMSE values from PCR vs Linear Regression")
plt.xlabel("Number of principal components in PCR")
plt.ylabel("RMSE")
plt.xticks(np.arange(X_pc_train.shape[1]), np.arange(1, X_pc_train.shape[1]+1))

plt.legend()
plt.show()


# Plot R2 vs count of principal components used
plt.plot(r2_score_train_pcr, "-o", label="PCR $R^2_{train}$")
plt.plot(r2_score_test_pcr, "-o", label="PCR $R^2_{test}$")

# Plot horizontal lines at the values of the full linear regression model
plt.axhline(y=r2_score_train, color='tab:blue', linestyle='--', label="LR $R^2_{train}$")
plt.axhline(y=r2_score_test, color='tab:orange', linestyle='--', label="LR $R^2_{test}$")

plt.title("$R^2$ values from PCR vs Linear Regression")
plt.xlabel("Number of principal components in PCR")
plt.ylabel("$R^2$")
plt.xticks(np.arange(X_pc_train.shape[1]), np.arange(1, X_pc_train.shape[1]+1))

plt.legend()
plt.show()


display(Markdown("The plots above indicate that a PCR on the 4 first principal components achieves results that are close to the full linear regression, with a much smaller number of predictors. A PCR with 6 principal components would also be a good alternative."))