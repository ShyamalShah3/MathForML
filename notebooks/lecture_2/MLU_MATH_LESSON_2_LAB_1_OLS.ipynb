{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<div style=\"background-image: linear-gradient(145deg, rgba(35, 47, 62, 1) 0%, rgba(0, 49, 129, 1) 40%, rgba(32, 116, 213, 1) 60%, rgba(244, 110, 197, 1) 85%, rgba(255, 173, 151, 1) 100%); padding: 1rem 2rem; width: 95%\"><img style=\"width: 60%;\" src=\"../../images/MLU_logo.png\"></div>\n",
    "\n",
    "# <a name=\"0\">MLU Mathematical Fundamentals for Machine Learning</a>\n",
    "# <a name=\"0\">Lecture 2: Advanced linear algebra</a>\n",
    "## <a name=\"0\">Lab 2.1: Ordinary least squares</a>\n",
    "\n",
    " 1. <a href=\"#1\">Business Problem: Checkout Amount?</a> \n",
    " 2. <a href=\"#2\">Linear models</a> \n",
    " 3. <a href=\"#3\">Ordinary least squares</a> \n",
    " 4. <a href=\"#4\">Scikit-learn implementation</a> \n",
    " 5. <a href=\"#5\">Multivariate linear regression</a> \n",
    "\n",
    "Linear models play a crucial role in machine learning due to their simplicity, interpretability, and effectiveness in capturing linear relationships between input features and target variables. \n",
    " - Many real-world problems exhibit non-linear patterns; however, linear models serve as a starting point and provide a **baseline** for understanding and evaluating more complex models. \n",
    " - By providing a clear relationship between input features and output predictions, linear models offer insights into **feature importance** that serve to gain a deeper understanding of the underlying problem. \n",
    " - Furthermore, the concepts underlying linear models form the foundation for many advanced machine learning techniques, most importantly **neural networks**. \n",
    "\n",
    "In this lab we will frame linear model in terms of **matrix and vector operations**. Posing a linear model in terms of matrix multiplication is useful because it allows us to take advantage of linear algebra’s efficient, well-studied computational methods. Representing a linear model as a matrix equation allows to express complex systems of equations compactly and solve them quickly, even for large datasets. This matrix form also makes it straightforward to perform operations like calculating derivatives for optimization, which is crucial for training models. Ultimately, framing linear models in terms of matrices enhances both the computational efficiency and conceptual clarity of model building and makes scaling up to high-dimensional data feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upgrade libraries\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a name=\"1\">1. Business Problem: Checkout Amount?</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "A product manager in an Amazon department examines customer purchase data, like: \n",
    "\n",
    "* __Visitor Tag__: Customer ID\n",
    "* __Product Category__: Gardening\n",
    "* __Spend in Category__: Visitor’s spend in the gardening product category for the 6 months prior to visiting\n",
    "* __Prime Member__:  Months as a Prime member\n",
    "* __Checkout Amount__:  Checkout amount within the gardening category at the end of that visit\n",
    "\n",
    "The manager would like to predict the ```Checkout Amount``` at the end of the customer's visit on the gardening product page, based on the amount the customer has spent in the category over the last 6 months (```Spend in Category```) or the amount of time in the Prime program (```Prime Member```).\n",
    "\n",
    "This is a __regression__ problem that can be handled with ML techniques such as __Linear Regression__. In this notebook you will apply the __Ordinary Least Squares (OLS)__ to find the best linear model. OLS is a very commonly used method for linear regression because it has desirable properties:\n",
    "\n",
    " - It provides an unbiased estimator for the coefficients (weights) of the model\n",
    " - It minimizes the variance in the estimates (under certain conditions)\n",
    " - It produces closed-form solutions, in many cases, through the normal equation\n",
    " \n",
    "#### Data Loading and Data Splitting\n",
    "\n",
    "First, let's load and inspect the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read csv containing dataset\n",
    "data = pd.read_csv(\"../../data/MATH_LAB_2_1_Data.csv\")\n",
    "\n",
    "display(Markdown(f\"Shape of the dataset: {data.shape}\"))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Before we start building a Machine Learning model, it is crucial to split the data into training and testing sets. The model will be trained on the training split. The test split allows to evaluate how well the model generalizes to unseen data. \n",
    "\n",
    "Let's use standard tools in `sklearn` to perform the data split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using train_test_split without further indications will assign 25% data to the test set\n",
    "data_train, data_test = train_test_split(data, random_state=222)\n",
    "\n",
    "display(Markdown(f\"Shape of the training split: {data_train.shape}\"))\n",
    "display(Markdown(f\"Shape of the test split: {data_test.shape}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a name=\"2\">2. Linear models</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Linear regression may be both the simplest and most popular among the standard tools for regression, flowing from a few simple assumptions. We assume that the relationship between the model features $x_1, ... x_n$ (```Spend in Category``` and ```Prime Member``` in our case) and the target $y$ (```Checkout Amount```), is linear, i.e. that the target can be expressed as a **weighted sum** of the features. \n",
    "\n",
    "Let's simplify the problem by considering first only one independent feature, ```Spend in Category```. In this case, the problem is a univariate linear regression. In that case, the linearity assumption can be simply written as, \n",
    "\n",
    "$$\\text{Checkout Amount} = b + \\text{Spend in Category} \\cdot w_{\\text{Spend in Category}}$$\n",
    " \n",
    "where $w_{\\text{Spent in Category}}$ is called the weight of the respective feature, and $b$ is called a bias (or offset or intercept). The weight determines the influence of the feature on our prediction, and the bias is independent of the features. $b$ just indicates the value of the prediction when all features are 0. The bias term improves the expressivity of the model, its usefulness being more apparent when one considers regressors that can be expressed in linearly related but nonproportional units (like for example Celsius and Fahrenheit units). \n",
    "\n",
    "Linear models are __easily interpretable__, in the sense that one unit increase in the feature, ```Spend in Category``` here, leads to an increase (or decrease) by $w_{\\text{Spend in Category}}$ units of the target ```Checkout Amount```.\n",
    "\n",
    "Also note that this is in fact the equation of a line! For univariate regression, solving the linear regression problem reduces to finding a line that best fits the data. In a more general situation with more regressor features, or multivariate regression, the solution takes the shape of an hyperplane that best fits the multidimensional dataset. \n",
    "\n",
    "There is a little more to it though. According to the model equation, the training datapoints should lie very close to the model line. They would lie exactly on the line if the target variable ```Checkout Amount``` was completely free of experimental error. However, because of the errors in the target variable (assumed as well-behaved Gaussian noise), an exact model cannot be determined, but it can be approximated by a fitted line. This fitted line is what we traditionally call a __linear regression model__.\n",
    "\n",
    "Let's employ some linear algebra notations to simplify the linear regression equation. This will also make it easier to extend beyond univariate regression, to multivariate regression. To start, we can collect the bias and the weight in one vector $\\mathbf{w}$,\n",
    "\n",
    "$$\\mathbf{w} = [b, w_{\\text{Spend in Category}}],$$\n",
    "\n",
    "by appending a column of ones to the original features matrix, here containing only one ```Spend in Category``` feature $x$, and thus creating the extended matrix $\\mathbf{X}$, also called the **design matrix**:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{pmatrix}\n",
    "1 & x_{11} \\\\\n",
    "1 & x_{21} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x_{n1} \\\\\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "where $n$ is the number of data samples. These are the inputs to our model. If we also denote our target outputs, the true values of the ```Checkout Amount```, as $\\mathbf{y}$, we can re-write our linear regression equation as\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{X}\\mathbf{w}.$$\n",
    "\n",
    "Thus, the goal of linear regression is to find the weights $\\mathbf{w}$ such that, on average, the predictions made according to our model best fit the true observed data $\\mathbf{y}$. In other words, __the $\\mathbf{w}$ vector is the linear regression model__. Note that for univariate regression, the $\\mathbf{w}$ components $b$ and $w_{\\text{Spend in Category}}$ are the slope and the intercept of the best fit line. \n",
    "\n",
    "The question is now, how do we find $\\mathbf{w}$, given $\\mathbf{X}$ and $\\mathbf{y}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## <a name=\"3\">3. Ordinary least squares</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Under some mathematical assumptions, and imposing the minimization of the Sum of Square Errors, the problem leads to the normal equation:\n",
    "\n",
    "$$\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{y}.$$\n",
    "\n",
    "This closed-form solution of linear regression is known as the **Ordinary Least Squares (OLS) solution**. The square matrix $\\mathbf{X}^T \\mathbf{X}$, which is the *covariance matrix* of $\\mathbf{X}$ (up to a dimensional scaler), is a $(k+1)\\times (k+1)$ square matrix, where $k$ is the number of features in the dataset. Ideally (assuming no collinearities), $\\mathbf{X}^T \\mathbf{X}$ has an inverse that is easy to compute to solve the normal equation. \n",
    "\n",
    "Let's apply the normal equation to build a univariante linear regression model, using for the moment only the first feature `Spend in Cateogry`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define features and target columns\n",
    "selected_features = [\"Spend in Category\"]\n",
    "target = [\"Checkout Amount\"]\n",
    "\n",
    "# Train data as numpy arrays\n",
    "X_train = data_train[selected_features].values\n",
    "y_train = data_train[target].values\n",
    "\n",
    "# Test data as numpy arrays\n",
    "X_test = data_test[selected_features].values\n",
    "y_test = data_test[target].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 80%; max-height:80%; margin: 5px;\" src=\"../../images/MLU_challenge.png\" alt=\"MLU challenge\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>It is your turn!</b></p>\n",
    "        <p><b>Exercise 1. Ordinary least squares.</b></p>\n",
    "        <p>Implement a function to compute the OLS solution by coding the normal equation. The function will take as inputs a matrix of features for the model and a target vector <b>y</b>. It will compute the design matrix <b>X</b> by prepending a column with 1's to the matrix of features and will return the vector of weights <b>w</b> as output. Output the solution of applying your OLS function to the <code>X_train</code> defined above as a variable named <code>weights</code>.</p>\n",
    "        <p>Implement another function to predict with your linear model. Name it <code>linear_model</code>. The function should take as inputs the vector of weights <b>b</b> and a matrix of features. The output will be the weighted sum of weights and features, i.e. the dot product, that gives the predictions of the linear model for the given input dataset.</p>\n",
    "        </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### END OF CODE ######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "<div style=\"align: left; border: 4px solid lightcoral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_question.png\" alt=\"MLU solution\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Challenge Help</b></p>\n",
    "        <p>You can use <code>np.append()</code> to assemble the design matrix joining the column vector with 1s to the rest of the features. The shape of the <code>weights</code> vector output by your OLS function needs to be (2, 1).</p>\n",
    "        <p>If you're stuck, remove the <code>#</code> before the <code>load</code> instruction in the next code cell to display a sample solution.</p>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/lab21_ex1_solutions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Plotting and evaluating the solution\n",
    "\n",
    "Once you have computed the `weights` of the model and defined a function `linear_model` to apply it to any input data, we can plot the best fit line on top of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Raise errors if variable and function from Exercise 1 are not defined\n",
    "if \"weights\" not in dir():\n",
    "    raise NameError(\"Please define a `weights` variable containing the solution to the linear regression model.\")\n",
    "if \"linear_model\" not in dir():\n",
    "    raise NameError(\"Please define a `linear_model` function to predict with the linear regression model.\")\n",
    "\n",
    "# Plot data and solution\n",
    "# Data\n",
    "plt.scatter(X_train, y_train, label=\"Training data\")\n",
    "# Model predictions over a line\n",
    "xfit = np.linspace(X_train.min(), X_train.max(), 10)\n",
    "yfit = linear_model(weights, xfit[:, np.newaxis])\n",
    "plt.plot(xfit, yfit, 'orange', label=\"Best fit\")\n",
    "plt.xlabel(\"Spend in Category\")\n",
    "plt.ylabel(\"Checkout Amount\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### $R^2$ coefficient\n",
    "To evaluate the goodness of fit, we can compute the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination). A “good” value for $R^2$ in a linear regression model \n",
    "depends heavily on the context and the field. In fields like physics or engineering, where relationships between variables are often more deterministic, $R^2$ values above 0.9 are often expected. In social sciences, medicine, or economics, where data tends to be more variable and affected by multiple unknown factors, an $R^2$ around 0.5–0.7 can be considered reasonable. In business applications, especially those with inherently high variability (e.g., consumer behavior), an $R^2$ of 0.3–0.5 might still be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the model to the train and test datasets\n",
    "y_hat_train = linear_model(weights, X_train)\n",
    "y_hat_test = linear_model(weights, X_test)\n",
    "\n",
    "print(f\"Weights:\\n{weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"R2 on training data: {r2_score(y_train, y_hat_train):.3f}\")\n",
    "print(f\"R2 on test data: {r2_score(y_test, y_hat_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Mean squared error\n",
    "\n",
    "Next we can compute the [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error) to get an indication of how well the model performs: lower MSE values indicate better model accuracy, as they imply that predictions are closer to the actual data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"MSE on training data: {mean_squared_error(y_train, y_hat_train):.2f}\")\n",
    "print(f\"MSE on test data: {mean_squared_error(y_test, y_hat_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## <a name=\"4\">4. Scikit-learn implementation</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Ordinary Least Squares (OLS) linear regression is implemented in scikit-learn through the `LinearRegression` class in the `linear_model` module. The sklearn implementation uses different algorithms to fit the regression model for numerical stability and efficiency.\n",
    "\n",
    "Let's compare if the sklearn solution arrives to the same solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use sklearn implementation\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(f\"sklearn LinearRegression model parameters: {lr.intercept_}, {lr.coef_}\")\n",
    "print(f\"Number of sklearn LinearRegression model parameters: {len(lr.intercept_) + len(lr.coef_[0])}\")\n",
    "print()\n",
    "print(f\"sklearn LinearRegression training R2: {r2_score(y_train, lr.predict(X_train)):.2f}\")\n",
    "print(f\"sklearn LinearRegression test R2: {r2_score(y_test, lr.predict(X_test)):.2f}\")\n",
    "print()\n",
    "print(f\"sklearn LinearRegression training MSE: {mean_squared_error(y_train, lr.predict(X_train)):.2f}\")\n",
    "print(f\"sklearn LinearRegression test MSE: {mean_squared_error(y_test, lr.predict(X_test)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## <a name=\"5\">5. Multivariate linear regression</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "The formulation of the linear problem in terms of matrices makes the extension to multiple input features straightforward. Thus we can apply all code above to multivariate linear regression. Given that the input data contains a second feature `Prime Member`, you can investigate whether this extra information yields a better linear regression model than the previous univariate model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 80%; max-height:80%; margin: 5px;\" src=\"../../images/MLU_challenge.png\" alt=\"MLU challenge\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>It is your turn!</b></p>\n",
    "        <p><b>Exercise 1. Multivariate regression.</b></p>\n",
    "        <p>Train a linear regression model using multiple features. You should be able to reuse most of the code written above. You will now need to regenerate the matrix of features including the 2 features present in the original data.</p>\n",
    "        <p>Which of the linear regression models perform better as predictor on unseen data?</p>\n",
    "        </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### END OF CODE ######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "<div style=\"align: left; border: 4px solid lightcoral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_question.png\" alt=\"MLU solution\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Challenge Help</b></p>\n",
    "        <p>If you're stuck, remove the <code>#</code> before the <code>load</code> instruction in the next code cell to display a sample solution.</p>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/lab21_ex2_solutions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: left; background-color:#330066; width:99%;\"> \n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_robot.png\" alt=\"MLU robot\" width=\"100\" height=\"100\"/>\n",
    "    <span style=\"color: white; padding-left: 10px; align: left; margin: 15px;\">\n",
    "        <h3>Congratulations!</h3>\n",
    "        You have completed Lab 2.1: Ordinary least squares of Lecture 2: Advanced linear algebra of MLU Mathematical Fundamentals of Machine Learning.\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
