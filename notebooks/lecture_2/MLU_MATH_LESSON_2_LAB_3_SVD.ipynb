{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<div style=\"background-image: linear-gradient(145deg, rgba(35, 47, 62, 1) 0%, rgba(0, 49, 129, 1) 40%, rgba(32, 116, 213, 1) 60%, rgba(244, 110, 197, 1) 85%, rgba(255, 173, 151, 1) 100%); padding: 1rem 2rem; width: 95%\"><img style=\"width: 60%;\" src=\"../../images/MLU_logo.png\"></div>\n",
    "\n",
    "# <a name=\"0\">MLU Mathematical Fundamentals for Machine Learning</a>\n",
    "# <a name=\"0\">Lecture 2: Advanced linear algebra</a>\n",
    "## <a name=\"0\">Lab 2.3: Image compression with SVD</a>\n",
    "\n",
    " 1. <a href=\"#1\">Singular Value Decomposition</a> \n",
    " 2. <a href=\"#2\">Low-rank approximation</a> \n",
    " 3. <a href=\"#3\">Image compression with SDV</a> \n",
    " \n",
    "**Singular Value Decomposition (SVD)** is a powerful mathematical technique used to analyze and simplify matrices. These could be matrices representing data in various fields, including computer graphics, data science, and machine learning. In this lab, you will explore SVD in the context of image compression. For that application, SVD can decompose an image matrix into the product of three simpler matrices, which can then be used to represent the image in a more compact form, reducing the amount of data needed to store or transmit the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade libraries\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## <a name=\"1\">1. Singular Value Decomposition</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "For any matrix $A$ of dimension $m\\times n$ that could contain an image, dataset, or any 2D structure, SVD decomposes it into the product of three simpler matrices:\n",
    "\n",
    "$$\n",
    "A = U\\Sigma V^T,\n",
    "$$\n",
    "\n",
    "where\n",
    " - $U$ is the orthogonal matrix of left singular vectors of the original data.\n",
    " - $\\Sigma$ is the diagonal matrix of singular values, which represent the \"strength\" or importance of each direction. The diagonal entries, called singular values, are non-negative and sorted in decreasing order. There are $r$ non-null singular values, where $r$ is the rank of the matrix $A$. This is the number of independent columns or rows of the matrix. The rank $r$ is at most as large as the minimum of $m$ and $n$.\n",
    " - $V$ is the orthogonal matrix of right singular vectors.\n",
    "\n",
    "Before applying SVD to the image compression problem, let's get familiar with the SVD implementation in `numpy`. \n",
    "\n",
    "We start by defining a matrix with 7 rows and 5 columns. This matrix could be for instance a dataset containing 7 observations from 7 users (rows), each of which has 5 features (columns). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 1, 1, 0, 0],\n",
    "              [3, 3, 3, 0, 0],\n",
    "              [4, 4, 4, 0, 0],\n",
    "              [0, 2, 0, 4, 4],\n",
    "              [0, 0, 0, 5, 5],\n",
    "              [0, 0, 0, 4, 4],\n",
    "              [0, 1, 0, 2, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define utility function to pretty print a matrix\n",
    "def pprint(matrix):\n",
    "    with np.printoptions(precision=2, suppress=True, formatter={'float': '{:0.2f}'.format}, linewidth=100):\n",
    "        print(matrix)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Original matrix A\"))\n",
    "pprint(A)\n",
    "print(f\"Shape of A: {A.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "The matrix $A$ clearly has rows that are dependent of each other, thus its rank will be smaller than the minimum between the number or rows ($n$, in this case 5) and the number of columns ($m$, in this case 7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Rank of A: {np.linalg.matrix_rank(A)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "SVD is implemented in `numpy` as `np.linalg.svd` that directly returns the three elements of the factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SVD\n",
    "U, S, VT = np.linalg.svd(A)\n",
    "print(f\"Shape of A: {A.shape}\")\n",
    "\n",
    "print(f\"\\nShape of U: {U.shape}\")\n",
    "print(f\"Shape of S: {S.shape}\")\n",
    "print(f\"Shape of VT: {VT.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "tags": []
   },
   "source": [
    "The only peculiarity is that S returned by `np.linalg.svd` is a vector containing the singular values. It has $r$ non-null entries, corresponding the the rank of $A$. The rest of the singular values up to $m$ or $n$ (whichever is smaller) are zero (up to a numerical rounding error). \n",
    "\n",
    "To turn S into a proper $m\\times n$ matrix we use `np.diag` for the first 5x5 elements and then add the corresponding number of zero rows to reach 7 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Sigma = np.zeros((A.shape[0], A.shape[1]))\n",
    "Sigma[:min(A.shape[0], A.shape[1]), :min(A.shape[0], A.shape[1])] = np.diag(S)\n",
    "print(f\"Shape of Sigma: {Sigma.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Matrix of left singular vectors U\"))\n",
    "pprint(U)\n",
    "print(f\"Shape of U: {U.shape}\")\n",
    "\n",
    "display(Markdown(\"#### Diagonal matrix of singular values $\\Sigma$\"))\n",
    "pprint(Sigma)\n",
    "print(f\"Shape of Sigma: {Sigma.shape}\")\n",
    "\n",
    "display(Markdown(\"#### Matrix of right singular vectors $V^T$\"))\n",
    "pprint(VT)\n",
    "print(f\"Shape of VT: {VT.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 80%; max-height:80%; margin: 5px;\" src=\"../../images/MLU_challenge.png\" alt=\"MLU challenge\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>It is your turn!</b></p>\n",
    "        <p><b>Exercise 1. Recover original matrix A from its SVD factors. Additionally, check that U and VT are orthogonal matrices.</b></p>\n",
    "        <p>Simply check that the SVD returned by numpy effectively factorizes A. Compute the product of the three factors U, Sigma, VT and double check that the result is equal to the original matrix.</p>\n",
    "        <p>Then check that the left and right singular vectors in matrices U and VT are orthogonal.</p> \n",
    "        </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### END OF CODE ######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "<div style=\"align: left; border: 4px solid lightcoral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_question.png\" alt=\"MLU solution\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Challenge Help</b></p>\n",
    "        <p>You can use <code>@</code> for matrix multiplication and <code>pprint</code> to easily display the resulting matrices.</p><p>To demonstrate that the left and right singular values are orthogonal, you can compute dot products. You can also make use of the condition for a matrix to be orthogonal: that its transponse is the same as its inverse.</p>\n",
    "        <p>If you're stuck, remove the <code>#</code> before the <code>load</code> instruction in the next code cell to display a sample solution.</p>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/lab23_ex1_solutions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## <a name=\"2\">2. Low-rank approximation</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "A low-rank approximation consists of approximating a matrix $A$ by another matrix with lower rank, i.e. with fewer linearly independent rows or columns. The goal of this technique is to capture the most significant information from the original matrix $A$ while reducing the complexity of its representation.\n",
    "\n",
    "Using the properties of the SVD we can propose a low-rank approximation of rank $k$. After performing SVD, we get the matrices $U$, \n",
    "$\\Sigma$, and $V^T$. By keeping only the largest $k$ singular values from matrix $\\Sigma$, along with the corresponding $k$ columns of $U$ and $V$, we can approximate the original matrix $A$ using a reduced form:\n",
    "\n",
    "$$\n",
    "A \\approx U_k \\Sigma_k V_k^T, \n",
    "$$\n",
    "\n",
    "where $U_k$, $\\Sigma_k$, and $V_k^T$ contain only the first $k$ columns or entries. \n",
    "\n",
    "The largest singular values capture the most significant information about the data in $A$, while smaller singular values contribute less. This property is the foundation of SVD’s use for compressing data, for instance images.\n",
    "\n",
    "### Rank-1 approximation\n",
    "\n",
    "The most extreme case is to take only the largest singular value in $\\Sigma$, alongside the first left singular vector and the first right singular vector. In that case, matrix $A$ is approximated by the product of a $m\\times 1$ vector $u_1$, the first singular value $\\sigma_1$, and a $1\\times n$ vector $v_1^T$: \n",
    "\n",
    "$$\n",
    "A \\approx A_1 = u_1 \\sigma_1 v_1^T.  \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"#### First left singular vector $u_1$\"))\n",
    "pprint(U[:,:1])\n",
    "print(f\"Shape of U[:,:1]: {U[:,:1].shape}\")\n",
    "\n",
    "display(Markdown(\"#### Largest singular value $\\sigma_1$\"))\n",
    "pprint(Sigma[:1,:1])\n",
    "print(f\"Shape of Sigma[:1,:1]: {Sigma[:1,:1].shape}\")\n",
    "\n",
    "display(Markdown(\"#### First right singular vector $v_1^T$\"))\n",
    "pprint(VT[:1,:])\n",
    "print(f\"Shape of VT[:1,:]: {VT[:1,:].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rank-1 approximation\n",
    "\n",
    "r = 1\n",
    "A_1 = U[:, :r] @ Sigma[:r, :r] @VT[:r,:]\n",
    "display(Markdown(\"#### Rank-1 approximation $A_1$\"))\n",
    "pprint(A_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "We can compute how much of the total energy, defined by the sum of all singular values, is accounted for by the first singular value and how different the matrix $A_1$ is from $A$ as given by the Frobenius norm (essentially the $L_2$ distance for matrices) of the difference $A-A_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Fraction of total energy in first singular value\"))\n",
    "print(f\"{Sigma[:1,:1].diagonal().sum()/Sigma.sum():0.4f}\")\n",
    "\n",
    "display(Markdown(\"#### Frobenius norm of the difference $A-A_1$\"))\n",
    "print(f\"{np.linalg.norm(A-A_1):0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "In fact, let's plot how much cumulative energy is present in the first 1, 2, 3, etc singular values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(Sigma.diagonal())/Sigma.diagonal().sum())\n",
    "plt.title(\"Singular Values: Cumulative Sum\")\n",
    "plt.xlabel(\"Number of singular values\")\n",
    "plt.ylabel(\"Fraction of the total sum of singular values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Rank-2 approximation\n",
    "\n",
    "From the plot above it seems that if we took the first two singular values, the approximation will retain most of the total energy. In that case, matrix $A$ is approximated by the sum of two terms: \n",
    "\n",
    "$$\n",
    "A \\approx A_2 = u_1 \\sigma_1 v_1^T + u_2 \\sigma_2 v_2^T.  \n",
    "$$\n",
    "\n",
    "Let's try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"#### First two left singular vectors $u_1, u_2$\"))\n",
    "pprint(U[:,:2])\n",
    "print(f\"Shape of U[:,:2]: {U[:,:2].shape}\")\n",
    "\n",
    "display(Markdown(\"#### Two largest singular values $\\sigma_1, \\sigma_2$\"))\n",
    "pprint(Sigma[:2,:2])\n",
    "print(f\"Shape of Sigma[:2,:2]: {Sigma[:2,:2].shape}\")\n",
    "\n",
    "display(Markdown(\"#### First two right singular vectors $v_1^T, v_2^T$\"))\n",
    "pprint(VT[:2,:])\n",
    "print(f\"Shape of VT[:2,:]: {VT[:2,:].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rank-2 approximation\n",
    "\n",
    "r = 2\n",
    "A_2 = U[:, :r] @ Sigma[:r, :r] @VT[:r,:]\n",
    "display(Markdown(\"#### Rank-2 approximation $A_2$\"))\n",
    "pprint(A_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can see that $A_2$ is a good approximation of $A$ than $A_1$, which justifies a 2-rank decomposition for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Fraction of total energy in first two singular values\"))\n",
    "print(f\"{Sigma[:2,:2].diagonal().sum()/Sigma.sum():0.4f}\")\n",
    "\n",
    "display(Markdown(\"#### Frobenius norm of the difference $A-A_2$\"))\n",
    "print(f\"{np.linalg.norm(A-A_2):0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "The Frobenous norm of the difference is now much smaller than before, indicating that this rank-2 approximation better matches the original matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a name=\"3\">3. Image compression with SVD</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Many large datasets contain hidden structures that enable compact, low-rank representations, effectively making them compressible by nature. Images serve as a particularly illustrative example of this phenomenon. By representing an image as a matrix of pixel values, we can leverage these underlying patterns to achieve efficient compression while preserving essential visual information. In this section you will learn how SVD can be used to reduce the size needed to save an image by only accepting a small loss of quality.\n",
    "\n",
    "Images in color are typically represented as numpy arrays of shape (height, width, 3) where the third dimension represents the RGB channels. Each element is a value in the $0-255$ range for the respective color channel. To simplify this exercise you will work with a gray scale image, because then we have only one value for the colour intensity at each pixel rather than three.\n",
    "\n",
    "Let's load an image and turn it into grayscale by averaging the values of its 3 channels. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read color image\n",
    "A = plt.imread(\"../../data/MATH_LAB_2_3_Alpaca.jpg\")\n",
    "print(A.shape)\n",
    "\n",
    "# Convert to grayscale\n",
    "X = np.mean(A, axis=2)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility to plot the image\n",
    "def plot_image(data):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    img = plt.imshow(data)\n",
    "    img.set_cmap(\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Plot image\n",
    "plot_image(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 80%; max-height:80%; margin: 5px;\" src=\"../../images/MLU_challenge.png\" alt=\"MLU challenge\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>It is your turn!</b></p>\n",
    "        <p><b>Exercise 2. Perform SVD on the image and compress it.</b></p>\n",
    "        <p>Apply SVD to obtain the U, Sigma, and V matrices.</p>\n",
    "        <p>Plot the low-rank approximation of the image at ranks 5, 50, and 500.</p>\n",
    "        <p>Plot the cumulative sum of singular values and the image that contains 90% of the total.</p> \n",
    "        </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### END OF CODE ######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "<div style=\"align: left; border: 4px solid lightcoral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_question.png\" alt=\"MLU solution\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Challenge Help</b></p>\n",
    "        <p>You can use <code>np.where()</code> to find the index where an array fulfils a certain condition.</p>\n",
    "        <p>If you're stuck, remove the <code>#</code> before the <code>load</code> instruction in the next code cell to display a sample solution.</p>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/lab23_ex2_solutions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: left; background-color:#330066; width:99%;\"> \n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_robot.png\" alt=\"MLU robot\" width=\"100\" height=\"100\"/>\n",
    "    <span style=\"color: white; padding-left: 10px; align: left; margin: 15px;\">\n",
    "        <h3>Congratulations!</h3>\n",
    "        You have completed Lab 2.3: Image compression with SVD of Lecture 2: Advanced linear algebra of MLU Mathematical Fundamentals of Machine Learning.\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
