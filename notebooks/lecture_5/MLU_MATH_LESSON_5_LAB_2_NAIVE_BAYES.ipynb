{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-image: linear-gradient(145deg, rgba(35, 47, 62, 1) 0%, rgba(0, 49, 129, 1) 40%, rgba(32, 116, 213, 1) 60%, rgba(244, 110, 197, 1) 85%, rgba(255, 173, 151, 1) 100%); padding: 1rem 2rem; width: 95%\"><img style=\"width: 60%;\" src=\"../../images/MLU_logo.png\"></div>\n",
    "\n",
    "# <a name=\"0\">MLU Mathematical Fundamentals for Machine Learning</a>\n",
    "# <a name=\"0\">Lecture 5: Probability and Statistics Applications</a>\n",
    "## <a name=\"0\">Lab 5.2: Naïve Bayes</a>\n",
    "\n",
    " 1. <a href=\"#1\">Business Problem: Frand Detection</a> \n",
    " 2. <a href=\"#2\">Naive Bayes</a> \n",
    " 3. <a href=\"#3\">Naive Bayes sklearn Implementation</a> \n",
    " 4. <a href=\"#4\">Summary</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "# Upgrade dependencies\n",
    "#!pip install --upgrade pip\n",
    "#!pip install --upgrade scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math as math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"1\">1. Business Problem: Fraud Detection</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "A product manager examines customer data to predict __Fraudulent__ activities, based on the following variables collected: __Grammar__, __Cheapest__, __Country__, __Picture__, and __Review__. \n",
    "\n",
    "This is a __classification__ problem that can be handled, aside from the other ML techniques discussed previously, with a Naive Bayes Classifier.\n",
    "\n",
    "<img style=\"width: 30%;\" src=\"../../images/fraud_detection.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the dataset and print the first five examples\n",
    "data = pd.read_csv('../../data/MATH_Lecture_5_Data.csv')\n",
    "data.columns\n",
    "\n",
    "model_features = ['Grammar', 'Cheapest', 'Country', 'Picture', 'Review']\n",
    "data = data.drop('Customer ID', axis = 1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, random_state = 0)\n",
    "print('Train shape: {}; test shape: {}'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"2\">2. Naive Bayes</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "The Naive Bayes is linear classifier leveraging the Bayes Theorem to build a probabilistic model to predict a class label $y$ for some given data observation $Data$:\n",
    "\n",
    "$$P(y|Data) = \\frac{P(Data|y) P(y)}{P(Data)},$$\n",
    "\n",
    "That is, it aims to estimate the conditional probability of a class label given the observed data, by finding the probability of class - which we can estimate from the labeled dataset, and most importantly, the conditional probability of the data observed based on the class label. For Bayes Theorem to succed computing this conditional probability, especially when each data input variable is dependent on all other variables, a large number of observed data samples are needed to figure out reliable probabilities distributions for all different possible combinations of variables involved. Even when such large datasets are available, (as we remarked during the lecture) Bayes Theorem itself is computationally expensive. \n",
    "\n",
    "However, the calculation of Bayes Theorem can be simplified with some *naive* assumptions on the input variables: consider each __input variable as being independent from each other__; and also consider that __all the input variables have an equal effect on the outcome__. Under these assumptions, the conditional probability of all input data variables given the class label becomes a product of all separate conditional probabilities of each variable value given the class label. \n",
    "\n",
    "Overall, the estimates of the conditional probabilities for each class label given the observed data are compared, and the class label with the largest probability is used for final class assignment for the given data instance.\n",
    "\n",
    "Let's see how a __Naive Bayes__ classifier works in practice, for our binary classification problem with five categorical (encoded binary) input variables, ```Grammar``` $(Gr)$, ```Cheapest``` $(Ch)$, ```Country``` $(Co)$, ```Picture``` $(Pi)$, and ```Review``` $(Re)$. Under the Naive Bayes assumptions, the conditional probability of class $y$ = ```Fraudulent``` $(1)$ given the observed data would be:\n",
    "\n",
    "\n",
    "$$\n",
    "P(y = 1|Gr, Ch, Co, Pi, Re) = \\frac{P({Gr}|y = 1) P({Ch}|y = 1) P({Co}|y = 1) P({Pi}|y = 1) P({Re}|y = 1) P(y = 1)}{P(Gr, Ch, Co, Pi, Re)},\n",
    "$$\n",
    "\n",
    "\n",
    "with a similar computation for the other class, $(0)$. Because $P(Gr, Ch, Co, Pi, Re)$ will be the same regardless the class, and only has the effect of normalizing the results, to further simplify the computations, we drop the $P(Gr, Ch, Co, Pi, Re)$ from the hunt for the conditional probability specific to each class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize dataset by class, compute class probabilities\n",
    "\n",
    "Let's first summarize our train dataset by class, and compute the two class probabilities. The dataset is first split by class, then class probabilities are calculated based on counts from each subset, using\n",
    "\n",
    "$$\n",
    "P(y = 0) = \\frac{count(y = 0)}{count(y = 0) + count(y = 1)},\n",
    "$$ \n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "P(y = 1) = \\frac{count(y = 1)}{count(y = 0) + count(y = 1)}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class probabilities \n",
    "def class_probabilities(train):\n",
    "    class_count = [train[train['Fraudulent'] == i].shape[0] for i in [0,1]]\n",
    "\n",
    "    class_proba = [class_count[i]/(np.sum(class_count)) for i in [0, 1]]\n",
    "    \n",
    "    return class_count, class_proba\n",
    "\n",
    "class_count, class_proba = class_probabilities(train)\n",
    "print(class_count, class_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize class subsets by variable values, compute conditional probabilities\n",
    "\n",
    "The conditional probabilities are the probabilities of each input variables given each class value, and can be calculated by splitting each class specific subset further into variables values specific subsets, and counting the elements of each subsets. For example, for ```Grammar``` the conditional probabilities will be given by\n",
    "\n",
    "$$\n",
    "P({Gr} = 1|y = 1) = {P({Gr} = 1, y = 1)}/{P(y = 1)}, \\\\\n",
    "P({Gr} = 0|y = 1) = {P({Gr} = 0, y = 1)}/{P(y = 1)}, \\\\\n",
    "P({Gr} = 1|y = 0) = {P({Gr} = 1, y = 0)}/{P(y = 0)}, \\\\\n",
    "P({Gr} = 0|y = 0) = {P({Gr} = 0, y = 0)}/{P(y = 0)}, \\\\\n",
    "$$\n",
    "\n",
    "with similar computations for the other four variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Smoothing__: If a given pair of variable value and class label never occur together in the training data, then the conditional probability estimate for that variable value conditioned on that class will be zero. One way to avoid zero probabilities, is to introduce an [additive (Laplace) smoothing](https://en.wikipedia.org/wiki/Additive_smoothing). We choose to code in a very simple approach to additive smoothing below, adding $1$ to each of the counts - we assume we saw each variable value for each class once more than we actually did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the conditional probabilities \n",
    "def variables_conditional_probabilities(train, smoothing):\n",
    "    vars_proba = []\n",
    "    for var in model_features:\n",
    "        var_proba = []\n",
    "        for class_value in [0, 1]:\n",
    "            data_class_value = train[train['Fraudulent'] == class_value]\n",
    "            var_count = [data_class_value[data_class_value[var] == val].shape[0] for val in [0, 1]]\n",
    "            for val in [0, 1]:                     \n",
    "                var_y_proba = (var_count[val] + smoothing)/(class_count[class_value] + 2*smoothing)\n",
    "                var_proba.append(var_y_proba)\n",
    "        vars_proba.append(var_proba)\n",
    "    \n",
    "    return vars_proba\n",
    "    \n",
    "vars_proba = variables_conditional_probabilities(train, smoothing = 1)\n",
    "print(vars_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make class predictions with Naive Bayes\n",
    "\n",
    "We now have all the ingredients to compute the conditional probabilities for each class, and decide on final class assignmet based on largest probability value. Let's see how this works for one datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 24\n",
    "datapoint = train.iloc[n][0:5]\n",
    "datapoint_label = train.iloc[n][5]\n",
    "\n",
    "def compute_nb_probas(datapoint): \n",
    "    predicted_probas = []\n",
    "    max_predicted_proba = 0.0\n",
    "    for class_value in [0, 1]:\n",
    "        nb_proba = class_proba[class_value]\n",
    "        for i, var in zip(range(5), model_features):\n",
    "            if datapoint[var] == 0:\n",
    "                nb_proba = nb_proba*vars_proba[i][2*class_value]\n",
    "            if datapoint[var] == 1:\n",
    "                nb_proba = nb_proba*vars_proba[i][2*class_value + 1]\n",
    "        predicted_probas.append(nb_proba)\n",
    "        if nb_proba > max_predicted_proba:\n",
    "            max_predicted_proba = nb_proba\n",
    "            predicted_class = class_value\n",
    "            \n",
    "    return predicted_probas, max_predicted_proba, predicted_class\n",
    "    \n",
    "predicted_probas, max_predicted_proba, predicted_class = compute_nb_probas(datapoint)\n",
    "\n",
    "print('P(y = 0 | {}) = {}'.format(datapoint.values, predicted_probas[0]))\n",
    "print('P(y = 1 | {}) = {}'.format(datapoint.values, predicted_probas[1]))\n",
    "print('Predicted class: y = {}'.format(predicted_class))\n",
    "print('True class: y = {}'.format(datapoint_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also split the dataset into training and test, train our model implemention on the train, and test the trained model on the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(y_true, y_predict):\n",
    "    return np.mean(y_true == y_predict)\n",
    "\n",
    "def Naive_Bayse_custom(train, test):\n",
    "    class_proba = class_probabilities(train)\n",
    "    vars_proba = variables_conditional_probabilities(train, smoothing = 0)\n",
    "    predictions = []\n",
    "    for i in range(test.shape[0]):\n",
    "        datapoint = test.iloc[i]\n",
    "        predicted_probas, max_predicted_proba, predicted_class = compute_nb_probas(datapoint)\n",
    "        predictions.append(predicted_class)\n",
    "    return predictions\n",
    "\n",
    "print(Accuracy(train['Fraudulent'], Naive_Bayse_custom(train, train)))\n",
    "print(Accuracy(test['Fraudulent'], Naive_Bayse_custom(train, test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"3\">3. Naive Bayes sklearn Implementation</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's compare our Naive Bayes' performance to sklearn's [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html) classifiers. The different Naive Bayes classifiers of sklearn differ mainly by the assumptions they make regarding the distribution of $P(variable = value | y = class)$. If the probability distribution for a variable is complex or unknown, it can be a good idea to use a kernel density estimator to approximate the distribution directly from the data samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for model in [GaussianNB(), MultinomialNB(), ComplementNB(), BernoulliNB(), CategoricalNB()]:\n",
    "    \n",
    "    model.fit(train[model_features], train['Fraudulent'])\n",
    "    train_predictions_sk = model.predict(train[model_features])\n",
    "    test_predictions_sk = model.predict(test[model_features])\n",
    "    print('{} model train accuracy: {}'.format(model, accuracy_score(train['Fraudulent'], train_predictions_sk)))\n",
    "    print('{} model test accuracy: {}'.format(model, accuracy_score(test['Fraudulent'], test_predictions_sk)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"4\">4. Summary</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Our Naive Bayes classifier performed as good as the sklearn implementations, but not very well overall. In particular, the MultinomialNB that implements the Naive Bayes algorithm for multinomially distributed data, worked best on training and test. Keep in mind that this is a synthetic dataset, and again we focus here on the techniques itself, and not on performance. However, this also gives us the opportunity to comment on the relevance and importance of Naive Bayes as yet another tool in the classifier's toolkit. \n",
    " \n",
    "In spite of their apparently over-simplified assumptions, Naive Bayes classifiers have worked quite well in many real-world situations (document classification, spam filtering). They require a small amount of training data to estimate the necessary parameters, and can therefore be extremely fast compared to more sophisticated methods. For example, a comparison of logistic regression and Naive Bayes, can be found [here](https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf).\n",
    "\n",
    "While reaching for and/or trying different classifiers on a particular dataset, it's worth remembering that Naive Bayes assumes the input variables are independent of each other. This works well most of the time, even when some or most of the variables are in fact dependent. Nevertheless, the performance of the algorithm degrades when the input variables are more dependent of each other.\n",
    "\n",
    "Also, related to data distribution assumptions of Naive Bayes, as new data becomes available - think new spam/no spam emails data coming in, it can be relatively straightforward to use this new data with the old data to update the estimates of the parameters for each variable’s probability distribution. This allows the model to easily make use of new data or the changing distributions of data over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 80%; max-height:80%; margin: 5px;\" src=\"../../images/MLU_challenge.png\" alt=\"MLU challenge\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Try it yourself!</b></p>\n",
    "        <p><b>Exercise 1.</b> Train a logistics regression classifier on the same training dataset and compute its accuracy on the training and test dataset, then compare it with the results obtained using Naïve Bayes.\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### END OF CODE ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lab52_ex1_solutions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: left; background-color:#330066; width:99%;\"> \n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_robot.png\" alt=\"MLU robot\" width=\"100\" height=\"100\"/>\n",
    "    <span style=\"color: white; padding-left: 10px; align: left; margin: 15px;\">\n",
    "        <h3>Congratulations!</h3>\n",
    "        You have completed Lab 5.2: Naïve Bayes of Lecture 5: Probability and Statistics Applications of MLU Mathematical Fundamentals of Machine Learning.\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
