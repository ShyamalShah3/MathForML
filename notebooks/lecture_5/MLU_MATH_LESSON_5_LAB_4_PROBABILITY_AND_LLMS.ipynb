{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-image: linear-gradient(145deg, rgba(35, 47, 62, 1) 0%, rgba(0, 49, 129, 1) 40%, rgba(32, 116, 213, 1) 60%, rgba(244, 110, 197, 1) 85%, rgba(255, 173, 151, 1) 100%); padding: 1rem 2rem; width: 95%\"><img style=\"width: 60%;\" src=\"../../images/MLU_logo.png\"></div>\n",
    "\n",
    "# <a name=\"0\">MLU Mathematical Fundamentals for Machine Learning</a>\n",
    "# <a name=\"0\">Lecture 5: Probability and Statistics Applications</a>\n",
    "## <a name=\"0\">Lab 5.4: Probability applied to LLM</a>\n",
    "\n",
    " 1. <a href=\"#1\">Generating text</a> \n",
    " 2. <a href=\"#2\">LLMs and probability distributions</a> \n",
    " 3. <a href=\"#3\">Entropy</a> \n",
    " 4. <a href=\"#4\">Language Model Evaluation using Perplexity</a> \n",
    " \n",
    " In this notebook, we will explore how probability distributions and conditional probabilities play a crucial role in the functioning of large language models (LLMs). We will use the [GPT-2 model](https://huggingface.co/docs/transformers/en/model_doc/gpt2) as our \"toy model\" to illustrate these concepts. GPT-2, developed by OpenAI, is a generative pre-trained transformer model that can generate coherent and contextually relevant text given an initial prompt.\n",
    "\n",
    "What you will learn:\n",
    "* Probability Distributions: Understand how GPT-2 models the probability distribution of the next token in a sequence.\n",
    "* Conditional Probabilities: Learn how the model uses the context provided by the input sequence to compute the conditional probability of each possible next token.\n",
    "* Perplexity: See how perplexity is used to measure the model's uncertainty in predicting the next token.\n",
    "* Entropy and Sampling: Explore how different sampling strategies (e.g., top-k sampling, nucleus sampling) affect the entropy and diversity of generated text.\n",
    "\n",
    "By the end of this notebook, you will have a deeper understanding of how LLMs leverage mathematical tools from probability theory to generate text and how these concepts are implemented in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iexDrR5uK9pb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch #PyTorch library for tensor operations\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "su-cIdJVUzFy",
    "outputId": "7e3f88b4-9695-4fcc-eecd-424859115402",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"1\">1. Generating text</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Large language models (LLMs) like GPT-2 work with **tokens**, which are the fundamental units of text. Tokens can represent various elements within a text, such as words, subwords, or punctuation. The model first **encodes** text into tokens, converting the input text into a sequence of token IDs that it can process. A token ID is a unique numerical identifier (vector or tensor) assigned to each token in the vocabulary of a language model.\n",
    "After generating the text, it decodes these token IDs back into human-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XCkXWkSzVBdu",
    "outputId": "37770c6b-fb16-49c8-d96f-830e36ae46a8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input text\n",
    "input_text = \"Machine Learning University\"\n",
    "\n",
    "# Tokenize input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "print(f\"Encoded text: {input_ids}\")\n",
    "\n",
    "# Inspect the tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Decode the tokens back to text\n",
    "decoded_text = tokenizer.decode(input_ids[0])\n",
    "print(\"Decoded text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we demonstrate how GPT-2 generates text continuations for a given input prompt using different sampling methods. \n",
    "Here's a brief overview of the steps:\n",
    "\n",
    "* Encode Input Text: Convert the input prompt into token IDs.\n",
    "* Create Attention Mask: it is a tool used by large language models to focus on relevant parts of the input text while ignoring irrelevant parts\n",
    "* Generate Text: Use the model to generate multiple sequences with diverse continuations. See below for an explanation of the main parameters.\n",
    "* Sampling Methods: Enable sampling to add diversity, and use top-k and top-p sampling to control the range of possible next tokens.\n",
    "\n",
    "### Parameters\n",
    "`do_sample`:\n",
    "\n",
    "When True, the model uses sampling to generate the next token. This introduces randomness into the text generation process, allowing for more diverse and creative outputs.\n",
    "Sampling means that instead of always picking the token with the highest probability (which would be deterministic), the model selects tokens based on their probability distribution. This makes the generation process less predictable and can produce a variety of different sequences even when given the same input text.\n",
    "\n",
    "`temperature`:\n",
    "\n",
    "Adjusts the randomness of predictions by scaling the logits before applying softmax.\n",
    "Lower values (e.g., 0.5) make the model more deterministic, while higher values (e.g., 1.5) introduce more randomness.\n",
    "\n",
    "`top_k` Sampling:\n",
    "\n",
    "Limits the sampling pool to the top K highest probability tokens.\n",
    "For example, with top_k=50, only the 50 most likely next tokens are considered for sampling.\n",
    "\n",
    "`top_p` (Nucleus) Sampling:\n",
    "\n",
    "Limits the sampling pool to the smallest set of tokens whose cumulative probability exceeds a threshold $p$.\n",
    "For example, with top_p=0.95, tokens are considered until their cumulative probability is at least 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cRxk5R23U05N",
    "outputId": "821cae04-0c0d-4234-9d8b-5179dadd5e70",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode input text\n",
    "input_text = \"The colors of the rainbow are\"\n",
    "\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "attention_mask = torch.ones_like(input_ids)  # this one is telling the model to pay full attention to every token in the input text. \n",
    "\n",
    "# Generate text using different sampling methods\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=20,                  # Maximum length of generated text\n",
    "    attention_mask=attention_mask,  # Pass attention mask\n",
    "    num_return_sequences=5,         # Number of sequences to generate\n",
    "    do_sample=True,                 # Enable sampling (adds diversity)\n",
    "    top_k=50,                       # Consider top 50 words (Top-K sampling)\n",
    "    top_p=0.95,                     # Consider words with cumulative prob ≥ 0.95 (Nucleus sampling)\n",
    "    temperature = 1,                # Adjusts randomness; lower values make output more deterministic (default is 1)\n",
    "    pad_token_id=tokenizer.eos_token_id  # Set pad token ID to EOS token ID\n",
    ")\n",
    "\n",
    "# Print generated samples\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(f\"{i+1}) {tokenizer.decode(sample_output, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary of a language model like GPT-2 refers to the set of all possible tokens (usually words, subwords, punctuation marks, and special tokens) that the model can understand and generate. It is essentially a predefined list that maps each token to a unique token ID. Here is how you can access it for gpt-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Access vocabulary as a dictionary\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(f\"Size of gpt-2 vocabulary = {len(vocab)}\")\n",
    "print(f\"Random sample from vocabulary: {', '.join(np.random.choice(list(vocab.keys()), 10))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"2\">2. LLMs and probability distributions</a> \n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large language model (LLM) is a probabilistic system designed to predict the next token in a sequence of text, conditioned on the tokens already present in the input: $P({\\rm next\\ token} | {\\rm context})$. It estimates the probability of each token in its vocabulary being the next token in the sequence, taking into account the context provided by the input string. This predictive capability allows the model to generate coherent and contextually relevant text, making it a versatile tool for various natural language processing tasks.\n",
    "\n",
    "Let's create a function that, given an input context (a text string), provides the conditional probability distribution of the next token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to predict probabilities of next token\n",
    "def next_token_probabilities(input_text):\n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Predict logits for next token\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        next_token_logits = outputs.logits[:, -1, :]  # Logits for last token in sequence\n",
    "\n",
    "    # Apply softmax to logits to get probabilities\n",
    "    next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "    # Convert token probabilities to Python list for easier manipulation\n",
    "    next_token_probs = next_token_probs.tolist()[0]\n",
    "\n",
    "    # Get the tokens corresponding to each probability\n",
    "    token_ids = range(len(next_token_probs))\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "    # Decode tokens to remove special characters like \"Ġ\"\n",
    "    tokens_decoded = [tokenizer.decode([token_id]) for token_id in token_ids]\n",
    "\n",
    "    # Create a list of dictionaries for each token and its probability\n",
    "    token_probabilities = [{'token': token_decoded, 'probability': prob} for token_decoded, prob in zip(tokens_decoded, next_token_probs)]\n",
    "\n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    df = pd.DataFrame(token_probabilities).sort_values(by='probability', ascending=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A clarification on the above function.\n",
    "\n",
    "Logits are the unconstrained output values of a neural network, before applying any activation function. In the context of a classification problem, the logits represent the raw, unscaled scores for each class. Mathematically, the logits can be represented as follows:\n",
    "\n",
    "$\\text{logits} = \\mathbf{z} = \\mathbf{W}^\\top \\mathbf{x} + \\mathbf{b}$\n",
    "\n",
    "Where:\n",
    "$\\mathbf{z}$ is the vector of logits, with each element corresponding to a class;\n",
    "$\\mathbf{W}$ is the weight matrix of the neural network;\n",
    "$\\mathbf{x}$ is the input vector;\n",
    "$\\mathbf{b}$ is the bias vector.\n",
    "\n",
    "#### Softmax\n",
    "The softmax function is an activation function used to convert logits into probabilities. It takes the logits as input and outputs a probability distribution over the classes.\n",
    "\n",
    "The softmax function is defined as:\n",
    "\n",
    "$\\text{softmax}({z_i}) = \\displaystyle{\\frac{\\exp(z_i)}{\\sum_{j=1}^K \\exp(z_j)}}$\n",
    "\n",
    "Where: $\\text{softmax}({z_i})$ is the probability of the $i$-th class;\n",
    "$z_i$ is the $i$-th logit value;\n",
    "$K$ is the number of classes.\n",
    "The softmax function has the following properties:\n",
    "\n",
    "* The output values are non-negative and sum up to 1, forming a valid probability distribution.\n",
    "* The function is differentiable, which is important for training neural networks using gradient-based optimization methods.\n",
    "* The softmax function can be interpreted as a \"soft\" version of the argmax function, which selects the class with the highest logit value.\n",
    "\n",
    "By applying the softmax function to the logits, we can obtain the probability distribution over the classes, which can be used for various classification tasks.\n",
    "\n",
    "Let's apply the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "input_text = \"The capital of France is\"\n",
    "df_probabilities = next_token_probabilities(input_text)\n",
    "\n",
    "# Print the DataFrame\n",
    "df_probabilities.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the probability distribution (mass function) for the next token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topn = 30\n",
    "df_top20 = df_probabilities.head(topn)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.bar(df_top20['token'], df_top20['probability'], color='skyblue')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(rotation=60)  # Rotate x labels for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the 'probability' column represents valid probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if np.isclose(df_probabilities.probability.sum(), 1, rtol=1e-3):\n",
    "    print(\"The 'probability' column represents valid probabilities.\")\n",
    "else:\n",
    "    print(\"The 'probability' column does not represent valid probabilities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot the distribution of probabilities, we will notice that there are a few token with high probabilities, while the majority of tokens have probabilities close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.hist(df_probabilities.probability, bins=300, log=True)\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Probabilities (Log Scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"3\">3. Entropy</a> \n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we quantify the uncertanty of the language model to perform the task of predicting the next word?\n",
    "One way to do this is to calculate the entropy of the probability distribution of the next token. Entropy is a measure of the average amount of information produced by a stochastic process, in this case, the distribution of token probabilities predicted by the language model.\n",
    "\n",
    "$$H(X) = -\\sum_i p(x_i)\\cdot {\\rm log}(p(x_i))$$\n",
    "\n",
    "where $p(x_i)$ is the probability of the $i$-th token $x_i$, and the sum is over all tokens in the distribution.\n",
    "\n",
    "Let's create a function that calculates the entropy of a probability distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_entropy(df):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a probability distribution given as a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame with columns 'token' and 'probability'.\n",
    "                         'probability' should contain probabilities of tokens.\n",
    "\n",
    "    Returns:\n",
    "    - float: Entropy value calculated from the probabilities in the DataFrame.\n",
    "    \"\"\"\n",
    "    return -np.sum(df['probability'] * np.log2(df['probability']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the entropy of the next token for 3 examples of context, from generic to more specific ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text1 = \"Once upon a time\"\n",
    "print(f\"Entropy: {calc_entropy(next_token_probabilities(input_text1)):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text2 = \"Guitar is a musical\"\n",
    "print(f\"Entropy: {calc_entropy(next_token_probabilities(input_text2)):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text3 = \"one two three four\"\n",
    "print(f\"Entropy: {calc_entropy(next_token_probabilities(input_text3)):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting entropy: a higher entropy value indicates greater uncertainty in the predictions. If the model is unsure about which token will come next (more evenly spread probabilities), the entropy will be higher. Conversely, if the model is very confident (one probability near 1 and others near 0), the entropy will be lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvhef9Ja1lG7"
   },
   "source": [
    "## <a name=\"4\">4. Language Model Evaluation using Perplexity</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "In the previous section, we focused on the entropy associated to the prediction of the next token. We now introduce _perplexity_, another concept associated to the model uncertainty in predictions, which is derived from entropy but expressed on a more intuitive scale. Perplexity quantifies how well a language model predicts a sample (sequence of tokens) based on its probability distribution over the next tokens.\n",
    "\n",
    "It is calculated by exponentiating the average negative log-likelihood (entropy) of the model's predictions for a sequence:\n",
    "\n",
    "$$\\text{Perplexity} = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^N \\log p(x_i)\\right)= \\left(\\prod_{i=1}^N p(x_i)\\right)^{-\\frac{1}{N}}$$\n",
    "Where \n",
    "$N$ is the number of tokens in the generated text; \n",
    "$x_i$ is the $i$-th token in the generated text; \n",
    "$p(x_i)$ is the conditional probability of the $i$-th token given the previous tokens.\n",
    "\n",
    "It can be thought of as a measure of how \"difficult\" it is for the model to predict the next word or sequence of words. A lower perplexity indicates indicates a higher confidence on a model's predictions, which reflects in capturing language patterns to generate coherent text and more nuances of human language.\n",
    "\n",
    "Here, we calculate perplexity for the entire sequence of predicted tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRs1RTLp4Hcz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    # Encode input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    # Calculate loss (negative log-likelihood)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss  # Mean negative log-likelihood per token\n",
    "\n",
    "    # Calculate perplexity\n",
    "    num_tokens = input_ids.size(1)  # Number of tokens in the input text\n",
    "    perplexity = torch.exp(loss / num_tokens)  # Adjust for number of tokens\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Negative Log-Likelihood (Loss): {loss:.4f}\")\n",
    "    print(f\"Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 80%; max-height:80%; margin: 5px;\" src=\"../../images/MLU_challenge.png\" alt=\"MLU challenge\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Try it yourself!</b></p>\n",
    "        <p><b>Exercise 1.</b>Use the model to generate three different texts using the prompt <i>\"The future of AI is\"</i> for the following temperature values: $0.1$, $1.0$ and $2.0$. Compute loss and perplexity in each case and compare the generated texts as well as these two metrics across the three executions.</p>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### END OF CODE ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lab54_ex1_solutions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"align: left; border: 4px solid lightcoral; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_question.png\" alt=\"MLU solution\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Extra Challenge</b></p>\n",
    "        <p>Here’re some ideas that you can next try.</p>\n",
    "        <p>It may also be interesting to use the code from the previous exercise to evaluate how perpelxity changes if we generate texts of different length. We suggest not to exceed <code>max_length=500</code> to keep execution time in the order of a few minutes.</p>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: left; background-color:#330066; width:99%;\"> \n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_robot.png\" alt=\"MLU robot\" width=\"100\" height=\"100\"/>\n",
    "    <span style=\"color: white; padding-left: 10px; align: left; margin: 15px;\">\n",
    "        <h3>Congratulations!</h3>\n",
    "        You have completed Lab 5.4: Probability applied to LLM of Lecture 5: Probability and Statistics Applications of MLU Mathematical Fundamentals of Machine Learning.\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
