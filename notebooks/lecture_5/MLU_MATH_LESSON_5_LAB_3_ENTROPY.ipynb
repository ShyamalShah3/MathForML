{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-image: linear-gradient(145deg, rgba(35, 47, 62, 1) 0%, rgba(0, 49, 129, 1) 40%, rgba(32, 116, 213, 1) 60%, rgba(244, 110, 197, 1) 85%, rgba(255, 173, 151, 1) 100%); padding: 1rem 2rem; width: 95%\"><img style=\"width: 60%;\" src=\"../../images/MLU_logo.png\"></div>\n",
    "\n",
    "# <a name=\"0\">MLU Mathematical Fundamentals for Machine Learning</a>\n",
    "# <a name=\"0\">Lecture 5: Probability and Statistics Applications</a>\n",
    "## <a name=\"0\">Lab 5.3: Entropy</a>\n",
    "\n",
    " 1. <a href=\"#1\">Entropy's definition</a> \n",
    " 2. <a href=\"#2\">Entropy in Decision Trees</a> \n",
    " \n",
    "In this short lab we'll recall the definition of Entropy and will see in practice how Entropy is used for training a decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "# Upgrade dependencies\n",
    "#!pip install --upgrade pip\n",
    "#!pip install --upgrade scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"1\">1. Entropy's definition</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "For any random variable $X$ that follows a probability distribution $P$ with a probability density function (PDF) or a probability mass function (PMF) $p(x)$, we measure the expected amount of information through *entropy* (or *Shannon entropy*) as follows:\n",
    "\n",
    "$$H(X) = - E_{x \\sim P} [\\log p(x)].$$\n",
    "\n",
    "To be specific, if $X$ is discrete, $$H(X) = - \\sum_i p_i \\log p_i \\textrm{, where } p_i = P(X_i).$$\n",
    "\n",
    "Otherwise, if $X$ is continuous, we also refer entropy as *differential entropy*\n",
    "\n",
    "$$H(X) = - \\int_x p(x) \\log p(x) \\; dx.$$\n",
    "\n",
    "We can caclulate entropy based on this definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(p):\n",
    "    entropy = - p * np.log2(p)\n",
    "    out = sum(entropy)\n",
    "    return out\n",
    "\n",
    "calc_entropy(np.array([0.1, 0.5, 0.1, 0.3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the <code>scipy.stats.entropy</code> function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(np.array([0.1, 0.5, 0.1, 0.3]), base=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "You may be curious: in the entropy definition, why do we use an expectation of a negative logarithm? Here are some intuitions.\n",
    "\n",
    "First, why do we use a *logarithm* function $\\log$? Suppose that $p(x) = f_1(x) f_2(x) \\ldots, f_n(x)$, where each component function $f_i(x)$ is independent from each other. This means that each $f_i(x)$ contributes independently to the total information obtained from $p(x)$. As discussed above, we want the entropy formula to be additive over independent random variables. Luckily, $\\log$ can naturally turn a product of probability distributions to a summation of the individual terms.\n",
    "\n",
    "Next, why do we use a *negative* $\\log$? Intuitively, more frequent events should contain less information than less common events, since we often gain more information from an unusual case than from an ordinary one. However, $\\log$ is monotonically increasing with the probabilities, and indeed negative for all values in $[0, 1]$.  We need to construct a monotonically decreasing relationship between the probability of events and their entropy, which will ideally be always positive. Hence, we add a negative sign in front of $\\log$ function.\n",
    "\n",
    "Last, where does the *expectation* function come from? Consider a random variable $X$. We can interpret the self-information ($-\\log(p)$) as the amount of *surprise* we have at seeing a particular outcome.  Indeed, as the probability approaches zero, the surprise becomes infinite.  Similarly, we can interpret the entropy as the average amount of surprise from observing $X$. For example, imagine that a slot machine system emits statistical independently symbols ${s_1, \\ldots, s_k}$ with probabilities ${p_1, \\ldots, p_k}$ respectively. Then the entropy of this system equals to the average self-information from observing each output, i.e.,\n",
    "\n",
    "$$H(S) = \\sum_i {p_i \\cdot I(s_i)} = - \\sum_i {p_i \\cdot \\log p_i}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 80%; max-height:80%; margin: 5px;\" src=\"../../images/MLU_challenge.png\" alt=\"MLU challenge\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Try it yourself!</b></p>\n",
    "        <p><b>Exercise 1.</b>Compute the Entropy of the probability distribution explored in lecture 3: $X \\sim \\text{sum of rolling two dice}$. The probability distribution is given below.</p>\n",
    "                    <table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th scope=\"col\">$x$</th>\n",
    "      <th scope=\"col\">$PMF$</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>$2$</td>\n",
    "      <td>$\\displaystyle{\\frac{1}{36}}$</td>\n",
    "    </tr>\n",
    "          <tr>\n",
    "      <td>$3$</td>\n",
    "      <td>$\\displaystyle{\\frac{2}{36}}$</td>\n",
    "    </tr>\n",
    "          <tr>\n",
    "      <td>$4$</td>\n",
    "      <td>$\\displaystyle{\\frac{3}{36}}$</td>\n",
    "    </tr>\n",
    "          <tr>\n",
    "      <td>$5$</td>\n",
    "      <td>$\\displaystyle{\\frac{4}{36}}$</td>\n",
    "    </tr>\n",
    "          <tr>\n",
    "      <td>$6$</td>\n",
    "      <td>$\\displaystyle{\\frac{5}{36}}$</td>\n",
    "    </tr>\n",
    "          <tr>\n",
    "      <td>$7$</td>\n",
    "      <td>$\\displaystyle{\\frac{6}{36}}$</td>\n",
    "    </tr>\n",
    "          <tr>\n",
    "      <td>$8$</td>\n",
    "      <td>$\\displaystyle{\\frac{5}{36}}$</td>\n",
    "    </tr>\n",
    "          <tr>\n",
    "      <td>$9$</td>\n",
    "      <td>$\\displaystyle{\\frac{4}{36}}$</td>\n",
    "    </tr>\n",
    "          <tr>\n",
    "      <td>$10$</td>\n",
    "      <td>$\\displaystyle{\\frac{3}{36}}$</td>\n",
    "    </tr>\n",
    "          <tr>\n",
    "      <td>$11$</td>\n",
    "      <td>$\\displaystyle{\\frac{2}{36}}$</td>\n",
    "    </tr>\n",
    "          <tr>\n",
    "      <td>$12$</td>\n",
    "      <td>$\\displaystyle{\\frac{1}{36}}$</td>\n",
    "    </tr>\n",
    "      </table>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### END OF CODE ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lab53_ex1_solutions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"2\">2. Entropy in Decision Trees</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Entropy is used in practice as one of the criteria to train decision trees. We will demonstrate this via a simple example based on a small loans data set of $14$ records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the dataset and print the first five examples\n",
    "df = pd.read_csv('../../data/MATH_Lecture_5_Loans.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loans data set has a number of features and the label is the *Risk* column, which can assume values high, medium and low.\n",
    "First off let's do some data cleaning to deal with missing data as well as convert categorical features into numerical features as Python decision trees classifiers can deal only with numerical data as of now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleansing / Preparation\n",
    "df[\"Risk\"] = df[\"Risk\"].map({\"high\":2, \"medium\":1, \"low\":0})\n",
    "df[\"Credit History\"] = df[\"Credit History\"].map({\"bad\":1, \"unknown\":0, \"good\":1})\n",
    "df[\"Income_high\"] = df[\"Income\"].map({\"< NOK 15K\":0,\"NOK 15K â€“ NOK 35K\":0,\"> NOK 35K\":1})\n",
    "df.drop([\"No\", \"Income\"], axis=1, inplace=True)\n",
    "df = pd.get_dummies(data=df,columns=[\"Credit History\", \"Debt\", \"Collateral\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have transformed all features as well as the label into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"Credit History_1\": \"Credit_History\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now prepare the data set of features (matrix $X$) and the labels (vector $y$) to train a decision tree classifier from <code>sklearn</code> to predict the *Risk* based on all the other features.\n",
    "Note that we specify <code>criterion='entropy'</code> while instantiating the <code>DecisionTreeClassifier</code> object to ensure the decision tree training uses the Entropy metric. There are other alternatives available such as the Gini impurity, which is the default criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Risk'], axis=1, inplace=False)\n",
    "y = df['Risk']\n",
    "\n",
    "dtree=DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is complete, let's now visualise the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(20,10))\n",
    "tree.plot_tree(dtree, feature_names=X.columns, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the application of the Entropy measure in practice, we can look at the first node of the tree. At this point all features are candidates for splitting and *Income_high* is chosen. This choice is due to the fact that *Income_high* yields the highest **Information Gain** compared to the other features *Credit_History*, *Debt_low* and *Collateral_yes*.\n",
    "\n",
    "Information Gain while training decision trees is computed at every split as the difference between the Entropy of the node before the split and after the split. Let's see how this calculation works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a dummy column of ones for counting purposes\n",
    "df['dummy_col'] = np.ones(df['Risk'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in X.columns:\n",
    "    gr_df = df[[c, 'Risk', 'dummy_col']].groupby([c, 'Risk']).sum().reset_index()\n",
    "\n",
    "    wh = 0\n",
    "    print(gr_df)\n",
    "    for v in gr_df[c].unique():\n",
    "        w = gr_df[gr_df[c]==v]['dummy_col']/gr_df[gr_df[c]==v]['dummy_col'].sum()\n",
    "        h = entropy(w, base=2)\n",
    "        print(f\"The entropy value for {c}={v} is {h}\")\n",
    "        wh += h * (gr_df[gr_df[c]==v]['dummy_col'].sum()/14)\n",
    "    \n",
    "    print(f\"The weighted entropy value for {c} is {wh}\")\n",
    "    \n",
    "    # Entropy before the split minus weighted average of the entropy for each value of the feature\n",
    "    ig = entropy([5/14, 3/14, 6/14], base=2) - wh\n",
    "    \n",
    "    print(f\"The information gain splitting by {c} is {ig}\")\n",
    "    print('\\n \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest information gain at the first split is obtained by leveraging *Income_high* as a splitting feature. We can see just based on the *Risk* label distribution alone the entropy before the split is equal to $\\text{entropy}{([\\displaystyle{\\frac{5}{14}}, \\displaystyle{\\frac{3}{14}}, \\displaystyle{\\frac{6}{14}}], \\text{base}=2)=1.531}$ while the entropy after each split is computed as the weighted average of the inidividual entropies for each variable of the splitting feature, as explained in the slides and implemented in the above code snippet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: left; background-color:#330066; width:99%;\"> \n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_robot.png\" alt=\"MLU robot\" width=\"100\" height=\"100\"/>\n",
    "    <span style=\"color: white; padding-left: 10px; align: left; margin: 15px;\">\n",
    "        <h3>Congratulations!</h3>\n",
    "        You have completed Lab 5.3: Entropy of Lecture 5: Probability and Statistics Applications of MLU Mathematical Fundamentals of Machine Learning.\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
