{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-image: linear-gradient(145deg, rgba(35, 47, 62, 1) 0%, rgba(0, 49, 129, 1) 40%, rgba(32, 116, 213, 1) 60%, rgba(244, 110, 197, 1) 85%, rgba(255, 173, 151, 1) 100%); padding: 1rem 2rem; width: 95%\"><img style=\"width: 60%;\" src=\"../../images/MLU_logo.png\"></div>\n",
    "\n",
    "# <a name=\"0\">MLU Mathematical Fundamentals for Machine Learning</a>\n",
    "# <a name=\"0\">Lecture 5: Probability and Statistics Applications</a>\n",
    "## <a name=\"0\">Lab 5.1: Hypothesis Testing</a>\n",
    "\n",
    " 1. <a href=\"#1\">Business Problem: Hypothesis Testing</a> \n",
    " 2. <a href=\"#2\">Data sampling: Bernoulli Distribution</a> \n",
    " 3. <a href=\"#3\">Test Statistic</a> \n",
    " 4. <a href=\"#4\">Decision</a> \n",
    " 5. <a href=\"#5\">A computational approach: methodology</a> \n",
    " 6. <a href=\"#6\">A computational approach: example</a> \n",
    " \n",
    "In this notebook, we will walk through the process of analyzing an A/B test to evaluate whether a new feature on Amazon's product page leads to a higher conversion rate. The steps we will follow include formulating hypotheses, conducting power analysis to determine the appropriate sample size, performing the A/B test, and interpreting the results.\n",
    "\n",
    "In the last two sections we'll also introduce a computational approach to hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "# Upgrade dependencies\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q --upgrade seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.stats.api as sms\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from statsmodels.stats.proportion import proportions_ztest, proportion_confint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "\n",
    "random_state = 8 # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"1\">1. Business Problem: A/B Testing</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Would it be better to have a row of other recommendations instead of “Amazon’s Choice” on the website? How can we decide if this alternative will lead to more purchases? We can put another version of the site in production and see which version has a higher rate of visitors purchasing. Let's see how this can be approached leveraging statistical hypothesis testing.\n",
    "\n",
    "<img style=\"width: 60%;\" src=\"../../images/AB_testing.png\"></div>\n",
    "\n",
    "### Background\n",
    "Conversion rate optimization is a critical aspect of e-commerce, as even small improvements can lead to significant revenue increases. For this study, we assume that the standard conversion rate for Amazon's product page is $p_0 = 10\\%$. Our goal is to test if introducing a new feature can increase the conversion rate by at least $2\\%$, bringing it to $12\\%$.\n",
    "\n",
    "### Hypotheses\n",
    "* Null Hypothesis: The new feature does not increase the conversion rate. \n",
    "    * $H_0$: $p=p_0$\n",
    "* Alternative Hypothesis: The new feature increases the conversion rate.  \n",
    "    * $H_1$: $p>p_0$\n",
    "\n",
    "### Criteria\n",
    "We choose the industry standard for significance level. \n",
    "* confidence level: $1-\\alpha = 95$%\n",
    "* significance level: $\\alpha = 5$%\n",
    "\n",
    "The significance level $\\alpha$ represents the probability of rejecting the null hypothesis when it is actually true. It is the threshold for determining whether the observed effect is statistically significant.\n",
    "\n",
    "For this A/B test, we have set the significance level to $5\\%$, which means:\n",
    "\n",
    "_\"I am going to accept the result as not due to chance if, assuming no effect (i.e., the null hypothesis is true), I would obtain such a result only 5% of the time (once every 20 experiments).\"_\n",
    "\n",
    "This means that there is a 5% risk of concluding that the new feature has an effect on the conversion rate when, in fact, it does not (a Type I error).\n",
    "\n",
    "\n",
    "### Power analysis\n",
    "\n",
    "To proceed, we will use power analysis to determine the necessary **sample size** for our A/B test. This is crucial to ensure that the test has sufficient power to detect a meaningful difference between the two groups (control and treatment).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the power analysis\n",
    "effect_size = sms.proportion_effectsize(0.10, 0.12)  # Calculate effect size based on conversion rates\n",
    "alpha = 0.05  # Significance level\n",
    "power = 0.8  # Desired power of the test (typically 0.8 or 80%)\n",
    "\n",
    "# Calculate the sample size\n",
    "sample_size = sms.NormalIndPower().solve_power(effect_size, power=power, alpha=alpha, ratio=1)\n",
    "sample_size = int(sample_size)\n",
    "print(f\"Required sample size per group: {sample_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"2\">2. Data sampling: Bernoulli distribution</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We can model a visitor’s chance of purchasing as a Bernoulli distribution with $p$ being the chance of success (i.e. user purchases). A Bernoulli distribution has only two possible outcomes, namely 1 (success) and 0 (failure), and a single trial, for example, a visitor purchasing a product. So the random variable which has a Bernoulli distribution can take value 1 with the probability of success, $p$, and the value 0 with the probability of failure $1−p$. \n",
    "\n",
    "<img style=\"width: 25%;\" src=\"../../images/AB_testing_tables.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Bernoulli distribution is a special case of the binomial distribution where a single trial is conducted (n=1), we can generate a Bernoulli distributed discrete random variable using <code>np.random.binomial(1, p, n)</code> method from the <code>Statsmodels</code> package, which takes $p$ (probability of success) as a shape parameter, and $n$ the number of times to repeat the trials. \n",
    "\n",
    "### Creating a synthetic sample\n",
    "Let's randomly generate a synthetic dataframe of customers, where we artificially impose a different proportion of conversion for treatment customers (set to $12\\%$, the effect we hope to detect) vs control customers ($10\\%$, which is the status quo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_conversion_rate = 0.10\n",
    "treatment_conversion_rate = 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data for control group\n",
    "np.random.seed(random_state)  # For reproducibility\n",
    "control_conversions = np.random.binomial(1, control_conversion_rate, sample_size)\n",
    "control_group = pd.DataFrame({\n",
    "    'group': ['control'] * sample_size,\n",
    "    'conversion': control_conversions\n",
    "})\n",
    "\n",
    "# Simulate data for treatment group\n",
    "treatment_conversions = np.random.binomial(1, treatment_conversion_rate, sample_size)\n",
    "treatment_group = pd.DataFrame({\n",
    "    'group': ['treatment'] * sample_size,\n",
    "    'conversion': treatment_conversions\n",
    "})\n",
    "\n",
    "# Combine both groups into a single DataFrame\n",
    "df = pd.concat([control_group, treatment_group], ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.sample(4, random_state=random_state+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this DataFrame, the index can be thought of as if it was the customer_id.\\\n",
    "Let's inspect if the conversion rates for the two groups are as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('group').agg({'conversion':'mean'}).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = df.groupby('group').agg(\n",
    "    n_customers=('conversion', 'count'),\n",
    "    conversion_rate=('conversion', 'mean'),\n",
    "    standard_error_mean=('conversion', lambda x: stats.sem(x, ddof=0)),  # Standard error of the mean\n",
    "    standard_deviation=('conversion', lambda x: np.std(x, ddof=0))  # Standard deviation\n",
    ").round(3)\n",
    "\n",
    "display(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"3\">3. Test Statistic</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "For a random Bernoulli variable, $X \\sim \\mathrm{Bernoulli}(p)$, while the expected value (mean) is $\\mu_X = p$, the variance is given by $\\sigma_X^2 = p(1-p)$ - and thus, the standard deviation is $\\sigma_X = \\sqrt{p(1-p)}$.\n",
    "\n",
    "The goal is to figure out whether there's a (significant) difference between the *control* and *test* (also called *treatment*) purchase rates.\n",
    "That could be:\n",
    "\n",
    "<img style=\"width: 40%;\" src=\"../../images/large_proportion_difference.png\"></div>\n",
    "\n",
    "or\n",
    "\n",
    "<img style=\"width: 40%;\" src=\"../../images/small_proportion_difference.png\"></div>\n",
    "\n",
    "That is, for the *control* and *test* scenarios considered above, we have \n",
    "\n",
    "$$\\hat{p}_c = 0.095, \\,\\,\\,\\,\\,\\, \\text{and} \\,\\,\\,\\,\\,\\, \\hat{p}_t = 0.116.$$\n",
    "\n",
    "Converting into a hypothesis statement, the null hypothesis to be disproved here would be\n",
    "\n",
    "$$p_c = p_t,$$\n",
    "\n",
    "meaning that nothing interesting happens with a new version of the website. The alternative hypothesis being that the purchase rates will be different, and so a new site presentation will make a difference, and statistical analysis will give us more insights on that.\n",
    "\n",
    "So how to do this? The variable of interest is the difference between the control and test,\n",
    "\n",
    "$$\\hat{p}_c - \\hat{p}_t .$$\n",
    "\n",
    "If the two samples are the same, the difference would be 0, and this new random variable would come from a normal distribution $\\cal{N}(0, \\displaystyle{\\sqrt{\\frac{\\sigma_c^2+\\sigma_p^2}{n}}})$, or equivalently, \n",
    "\n",
    "$$\\frac{\\hat{p}_c - \\hat{p}_t}{\\displaystyle{\\sqrt{\\frac{\\sigma_c^2+\\sigma_p^2}{n}}}}$$\n",
    "\n",
    "would come from a standard Gaussian $\\cal{N}(0, 1)$. Assuming a normal distribution and a large sample size, a $z$-test can be used to determine whether the new version of the website *significantly* increase conversion rate. \n",
    "\n",
    "Let's calculate the measured effect, which is our sample statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_effect = summary_stats.loc['treatment', 'conversion_rate'] - summary_stats.loc['control', 'conversion_rate']\n",
    "print(f'difference in conversion rate, sample statistic = {observed_effect:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"3\">4. Decision</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "To calculate the p-value for the hypothesis test we can use a two-sample z-test for proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform z-test for the difference in proportions\n",
    "control_count = summary_stats.loc['control', 'n_customers']\n",
    "treatment_count = summary_stats.loc['treatment', 'n_customers']\n",
    "control_success = df[df['group'] == 'control']['conversion'].sum()\n",
    "treatment_success = df[df['group'] == 'treatment']['conversion'].sum()\n",
    "\n",
    "count = np.array([treatment_success, control_success])\n",
    "nobs = np.array([treatment_count, control_count])\n",
    "\n",
    "z_stat, p_value = proportions_ztest(count, nobs, alternative='larger')\n",
    "\n",
    "print(f\"Z-statistic: {z_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Interpret the result\n",
    "if p_value < alpha:\n",
    "    print(\"p<α: rejecting the null hypothesis, the new feature likely increases the conversion rate.\")\n",
    "else:\n",
    "    print(\"p>α: failing to reject the null hypothesis, there is not enough evidence that the new feature increases the conversion rate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the confidence interval for the estimated difference in conversion rates, we need to compute the standard error for the difference in proportions and then use it to determine the confidence interval.\n",
    "\n",
    "Here is the code snippet to calculate and print the confidence interval for the difference in conversion rates between the treatment and control groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute proportions\n",
    "p_control = control_success / control_count\n",
    "p_treatment = treatment_success / treatment_count\n",
    "\n",
    "# Compute the difference in proportions\n",
    "diff = p_treatment - p_control\n",
    "\n",
    "# Compute the standard error of the difference\n",
    "se_diff = np.sqrt(p_treatment * (1 - p_treatment) / treatment_count + p_control * (1 - p_control) / control_count)\n",
    "\n",
    "# Calculate the confidence interval\n",
    "z_critical = stats.norm.ppf(1 - alpha / 2)\n",
    "ci_lower = diff - z_critical * se_diff\n",
    "ci_upper = diff + z_critical * se_diff\n",
    "\n",
    "print(f\"Confidence interval for the difference in conversion rates: ({ci_lower:.4f}, {ci_upper:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 80%; max-height:80%; margin: 5px;\" src=\"../../images/MLU_challenge.png\" alt=\"MLU challenge\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Try it yourself!</b></p>\n",
    "        <p><b>Exercise 1.</b> Since the proportions representing the purchase rate can be approximated with the mean of the Bernoulli sample, we can also apply a z-test that leverages the sample mean of control and test. Try to research the <code>statsmodels.stats</code> library, find the appropriate test and apply it to our scenario. Compare the results with the two-sample z-test for proportions previously obtained.</p>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### END OF CODE ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lab51_ex1_solutions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 80%; max-height:80%; margin: 5px;\" src=\"../../images/MLU_challenge.png\" alt=\"MLU challenge\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Try it yourself!</b></p>\n",
    "        <p><b>Exercise 2.</b> Modify Exercise 1 code in order to run a <b>two-tailed</b> test, which answers the question: is there a significant difference (no direction implied) between the control and test websites purchase rates?\n",
    "<i>Significant</i> is somewhat arbitrary. When a 95% confidence interval is considered, corresponding to a $z$-value of 1.96, the null hypothesis is not rejected if: \n",
    "\n",
    "$$-1.96 \\leq \\frac{\\hat{p}_c - \\hat{p}_t}{\\displaystyle{\\sqrt{\\frac{\\sigma_c^2+\\sigma_p^2}{n}}}} \\leq 1.96,$$\n",
    "\n",
    "and the null hypothesis is rejected if\n",
    "\n",
    "$$\\frac{\\hat{p}_c - \\hat{p}_t}{\\displaystyle{\\sqrt{\\frac{\\sigma_c^2+\\sigma_p^2}{n}}}} \\leq -1.96 \\,\\,\\,\\,\\,\\, \\text{or} \\,\\,\\,\\,\\,\\,\n",
    "\\frac{\\hat{p}_c - \\hat{p}_t}{\\displaystyle{\\sqrt{\\frac{\\sigma_c^2+\\sigma_p^2}{n}}}} \\geq 1.96.$$</p>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### END OF CODE ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lab51_ex2_solutions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"5\">5. A computational approach: methodology</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Navigating the vast array of statistical tests in textbooks can be daunting, but understanding the core logic behind them simplifies the process. All statistical tests aim to answer the same fundamental question: **is the observed effect genuine, or is it merely due to chance?**\n",
    "\n",
    "To address this, we establish two hypotheses: the null hypothesis ($H_0$), which assumes the effect is due to chance, and the alternative hypothesis ($H_A$), which assumes the effect is real.\n",
    "\n",
    "Ideally, we would calculate the probability of observing the effect under both hypotheses, $P(E|H_0)$ and $P(E|H_A)$. However, because formulating $H_A$ can be challenging, conventional hypothesis testing focuses on computing $P(E|H_0)$, known as the $p$-value. A small $p$-value suggests that the observed effect is unlikely to be due to chance, indicating it may be real.\n",
    "\n",
    "In essence, all statistical tests aim to calculate $p$-values efficiently. Traditionally, this was done by developing specific statistical methods tailored to the problem at hand, which allowed for simplified computation using statistical tables—an approach still commonly taught in statistics courses. However, with modern computational power, simulations offer a robust alternative to these traditional methods, enabling us to focus more on the core logic of statistics rather than on the mathematical shortcuts.\n",
    "\n",
    "Conceptually, hypothesis tests follow these steps:\n",
    "\n",
    "1. **Choose a Test Statistic ($s$)**: Calculate a test statistic from your dataset that quantifies the observed effect. This could be any value derived from the data that reflects the effect you're measuring. For example, if you're comparing two groups, the test statistic might be the difference in means. Implement a function `compute_statistic` that computes this test statistic from the data. Apply it to your experimental data to obtain your observed test statistic, denoted as $\\hat{s}$.\n",
    "\n",
    "2. **Define the Null Hypothesis ($H_0$)**: Establish a model assuming that the observed effect is not real. For instance, if comparing two groups, the null hypothesis would assume no difference between them. Implement a function `generate_data` that generates random datasets under the assumption of $H_0$.\n",
    "\n",
    "3. **Simulate a Large Number of Experiments**: Create a function `random_experiments` that replicates your experiment multiple times by generating random datasets with `generate_data`. For each iteration, compute the test statistic using compute_statistic and store the results in a list.\n",
    "\n",
    "4. **Estimate the $p$-value**: The p-value represents the probability of observing an effect as extreme as your data under the null hypothesis. To estimate this, calculate the fraction of simulated test statistics that are as extreme as or more extreme than your observed statistic $\\hat{s}$.\n",
    "\n",
    "Finally, if the p-value is smaller than your predefined significance level ($\\alpha$), you can conclude that the observed effect is unlikely to be due to chance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Define the function to compute the test statistic\n",
    "def compute_statistic(data):\n",
    "    treatment_mean = data[data['group'] == 'treatment']['conversion'].mean()\n",
    "    control_mean = data[data['group'] == 'control']['conversion'].mean()\n",
    "    return treatment_mean - control_mean\n",
    "\n",
    "# 2 - Define the function to generate random datasets assuming H0\n",
    "def generate_data(data):\n",
    "    combined_conversion = np.concatenate((data['conversion'][data['group'] == 'control'].values,\n",
    "                                          data['conversion'][data['group'] == 'treatment'].values))\n",
    "    np.random.shuffle(combined_conversion)\n",
    "    half_size = len(combined_conversion) // 2\n",
    "    new_control = combined_conversion[:half_size]\n",
    "    new_treatment = combined_conversion[half_size:]\n",
    "    \n",
    "    new_data = pd.DataFrame({\n",
    "        'group': ['control'] * half_size + ['treatment'] * half_size,\n",
    "        'conversion': np.concatenate((new_control, new_treatment))\n",
    "    })\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "# Define the function to replicate random experiments\n",
    "def random_experiments(data, num_experiments):\n",
    "    simulated_statistics = []\n",
    "    for _ in range(num_experiments):\n",
    "        simulated_data = generate_data(data)\n",
    "        simulated_stat = compute_statistic(simulated_data)\n",
    "        simulated_statistics.append(simulated_stat)\n",
    "    return simulated_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the actual experiment statistic\n",
    "observed_statistic = compute_statistic(df)\n",
    "print(f\"Observed statistic: {observed_statistic:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the simulations\n",
    "num_experiments = 10000\n",
    "simulated_statistics = random_experiments(df, num_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the p-value\n",
    "simulated_statistics = np.array(simulated_statistics)\n",
    "p_value = np.mean(simulated_statistics >= observed_statistic)\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Interpret the result\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis. The new feature increases the conversion rate.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. The new feature does not significantly increase the conversion rate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "sns.kdeplot(simulated_statistics, bw_adjust=0.5, fill=True, label='simulated statistics under H0')\n",
    "ymax = plt.gca().get_ylim()[1]\n",
    "plt.plot([observed_statistic,observed_statistic],[0,ymax/2],color='red', label='observed statistic')\n",
    "plt.xlabel('test statistic')\n",
    "plt.ylabel('frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"6\">6. A computational approach: example</a> \n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "###  Analyzing Customer Review Ratings and Purchase Categories\n",
    "Amazon wants to investigate whether there is a relationship between the rating given by customers (1 to 5 stars) and the category of the product they purchase (e.g., Electronics, Books, Clothing). Traditionally, this can be analyzed using a chi-squared test, but we will apply a simulation method to test for independence between these two categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# Define the number of samples\n",
    "num_samples = 1000\n",
    "\n",
    "# Define the categories and ratings\n",
    "categories = ['Electronics', 'Books', 'Clothing']\n",
    "ratings = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Create a dataset with significant dependence between categories and ratings\n",
    "# For example, Electronics might have higher ratings, Books have mid ratings, and Clothing have lower ratings\n",
    "\n",
    "# Initialize lists to store the generated data\n",
    "category_list = []\n",
    "rating_list = []\n",
    "\n",
    "# Generate the data\n",
    "for _ in range(num_samples):\n",
    "    category = np.random.choice(categories, p=[1/3, 1/3, 1/3])  # Category probabilities\n",
    "    if category == 'Electronics':\n",
    "        rating = np.random.choice(ratings, p=[0.15, 0.15, 0.32, 0.21, 0.17])  # Higher ratings\n",
    "    elif category == 'Books':\n",
    "        rating = np.random.choice(ratings, p=[0.125, 0.2, 0.35, 0.2, 0.125])  # Mid ratings\n",
    "    else:  # Clothing\n",
    "        rating = np.random.choice(ratings, p=[0.17, 0.21, 0.32, 0.15, 0.15])  # Lower ratings\n",
    "    \n",
    "    category_list.append(category)\n",
    "    rating_list.append(rating)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'rating': rating_list,\n",
    "    'category': category_list\n",
    "})\n",
    "display(df.head())\n",
    "# Create the contingency table\n",
    "contingency_table = pd.crosstab(df['rating'], df['category'])\n",
    "print(\"Observed Contingency Table:\")\n",
    "display(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visually inspect the distribution of ratings. Would you say that they are different, or differences are just due to change, because of the random nature of the sampling process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the contingency table\n",
    "contingency_table.plot(kind='bar', figsize=(7,3), colormap='viridis')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Rating Frequencies for Each Category')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **chi-square** test is the standard choice for analyzing the association between categorical variables because it is specifically designed to test for independence by comparing observed and expected frequencies in a contingency table, making it ideal for this type of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, observed_p, dof, expected = chi2_contingency(contingency_table)\n",
    "print(f\"Observed Chi-squared statistic: {chi2:.4f}, p-value: {observed_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerated_ct = pd.DataFrame(contingency_table.to_dict())\n",
    "chi2_contingency(regenerated_ct)\n",
    "regenerated_ct['rating'] = regenerated_ct.index\n",
    "regenerated_ct.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to do if you don't know that chi-square is the standard test to use in this case? You can apply the principles described before and use simulations.\n",
    "\n",
    "To this end, you can create a process that mimics the randomization or permutation testing method. This involves shuffling the data to create random datasets under the null hypothesis and then computing the test statistic for each shuffled dataset. The p-value is estimated based on how extreme the observed statistic is compared to the distribution of simulated statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistic(data):\n",
    "    \"\"\"\n",
    "    Compute the chi-squared statistic for the given dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: A DataFrame containing the data.\n",
    "    \n",
    "    Returns:\n",
    "    - The chi-squared statistic.\n",
    "    \"\"\"\n",
    "    contingency_table = pd.crosstab(data['rating'], data['category'])\n",
    "    statistic, _, _, _ = chi2_contingency(contingency_table)\n",
    "    return statistic\n",
    "\n",
    "def generate_data(data):\n",
    "    \"\"\"\n",
    "    Generate a random dataset under the null hypothesis by shuffling the 'category' column.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: A DataFrame containing the original data.\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame with the 'category' column shuffled.\n",
    "    \"\"\"\n",
    "    shuffled_data = data.copy()\n",
    "    shuffled_data['category'] = np.random.permutation(shuffled_data['category'])\n",
    "    return shuffled_data\n",
    "\n",
    "def random_experiments(data, num_experiments):\n",
    "    \"\"\"\n",
    "    Run multiple random experiments under the null hypothesis.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: A DataFrame containing the original data.\n",
    "    - num_experiments: The number of random experiments to perform.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of simulated test statistics.\n",
    "    \"\"\"\n",
    "    simulated_statistics = []\n",
    "    \n",
    "    for _ in range(num_experiments):\n",
    "        # Generate a random dataset under the null hypothesis\n",
    "        random_data = generate_data(data)\n",
    "        \n",
    "        # Compute the statistic for the random dataset\n",
    "        _statistic = compute_statistic(random_data)\n",
    "        \n",
    "        # Store the chi-squared statistic\n",
    "        simulated_statistics.append(_statistic)\n",
    "    \n",
    "    return simulated_statistics\n",
    "\n",
    "# Run the simulations\n",
    "num_experiments = 1000\n",
    "observed_statistic = compute_statistic(df)\n",
    "simulated_statistics = random_experiments(df, num_experiments)\n",
    "p_value = np.mean([sim_stat >= observed_statistic for sim_stat in simulated_statistics])\n",
    "print(f\"Simulated p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing compute_statistic with one that is not chi_square\n",
    "\n",
    "def compute_statistic(data):\n",
    "    \"\"\"\n",
    "    Compute a custom statistic based on the absolute differences in proportions between observed and expected data.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: A DataFrame containing the data.\n",
    "    \n",
    "    Returns:\n",
    "    - A statistic that measures the total deviation in rating distributions across categories.\n",
    "    \"\"\"\n",
    "    # Create the contingency table (observed counts)\n",
    "    # normalize='columns' provides rates proportions for each category\n",
    "    contingency_table = pd.crosstab(data['rating'], data['category'], normalize='columns')\n",
    "    #display(contingency_table)\n",
    "    \n",
    "    # Calculate overall rating distribution (expected under H0)\n",
    "    overall_proportions = data['rating'].value_counts(normalize=True).sort_index()\n",
    "    #display(overall_proportions)\n",
    "    \n",
    "    # Compute the statistic as the sum of absolute deviations from expected proportions\n",
    "    statistic = 0\n",
    "    for category in contingency_table.columns:\n",
    "        for rating in contingency_table.index:\n",
    "            observed_prop = contingency_table.at[rating, category]\n",
    "            expected_prop = overall_proportions[rating]\n",
    "            statistic += abs(observed_prop - expected_prop)\n",
    "    \n",
    "    return statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 1000\n",
    "observed_statistic = compute_statistic(df)\n",
    "simulated_statistics = random_experiments(df, num_experiments)\n",
    "p_value = np.mean([sim_stat >= observed_statistic for sim_stat in simulated_statistics])\n",
    "print(f\"Simulated p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "<div style=\"align: left; border: 4px solid cornflowerblue; text-align: left; margin: auto; padding-left: 20px; padding-right: 20px; width: 65%\">\n",
    "        <img style=\"float: left; max-width: 80%; max-height:80%; margin: 5px;\" src=\"../../images/MLU_challenge.png\" alt=\"MLU challenge\" width=12% height=12%/>\n",
    "    <span style=\"padding: 20px; align: left;\">\n",
    "        <p><b>Try it yourself!</b></p>\n",
    "        <p><b>Exercise 3.</b> Complete the notebook with the code that generates the frequency plot of the test statistics simulated under the null hypothesis $H_0$ and shows where the observed statistics lies.</p>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### YOUR CODE HERE ######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### END OF CODE ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lab51_ex3_solutions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: left; background-color:#330066; width:99%;\"> \n",
    "        <img style=\"float: left; max-width: 100%; max-height:100%; margin: 15px;\" src=\"../../images/MLU_robot.png\" alt=\"MLU robot\" width=\"100\" height=\"100\"/>\n",
    "    <span style=\"color: white; padding-left: 10px; align: left; margin: 15px;\">\n",
    "        <h3>Congratulations!</h3>\n",
    "        You have completed Lab 5.1: Hypothesis Testing of Lecture 5: Probability and Statistics Fundamentals of MLU Mathematical Fundamentals of Machine Learning.\n",
    "        <br/>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
